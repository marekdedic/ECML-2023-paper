
@article{kingma_adam:_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2020-06-25},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Learning, Computer Science - Machine Learning},
}

@article{chen_harp_2018,
	title = {{HARP}: {Hierarchical} {Representation} {Learning} for {Networks}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	shorttitle = {{HARP}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11849},
	language = {en},
	number = {1},
	urldate = {2021-05-25},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chen, Haochen and Perozzi, Bryan and Hu, Yifan and Skiena, Steven},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {network classification},
}

@inproceedings{schulz_mining_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Mining {Tree} {Patterns} with {Partially} {Injective} {Homomorphisms}},
	isbn = {978-3-030-10928-8},
	doi = {10.1007/978-3-030-10928-8_35},
	abstract = {One of the main differences between inductive logic programming (ILP) and graph mining lies in the pattern matching operator applied: While it is mainly defined by relational homomorphism (i.e., subsumption) in ILP, subgraph isomorphism is the most common pattern matching operator in graph mining. Using the fact that subgraph isomorphisms are injective homomorphisms, we bridge the gap between ILP and graph mining by considering a natural transition from homomorphisms to subgraph isomorphisms that is defined by partially injective homomorphisms, i.e., which require injectivity only for subsets of the vertex pairs in the pattern. Utilizing positive complexity results on deciding homomorphisms from bounded tree-width graphs, we present an algorithm mining frequent trees from arbitrary graphs w.r.t. partially injective homomorphisms. Our experimental results show that the predictive performance of the patterns obtained is comparable to that of ordinary frequent subgraphs. Thus, by preserving much from the advantageous properties of homomorphisms and subgraph isomorphisms, our approach provides a trade-off between efficiency and predictive power.},
	language = {en},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer International Publishing},
	author = {Schulz, Till Hendrik and Horváth, Tamás and Welke, Pascal and Wrobel, Stefan},
	editor = {Berlingerio, Michele and Bonchi, Francesco and Gärtner, Thomas and Hurley, Neil and Ifrim, Georgiana},
	year = {2019},
	pages = {585--601},
}

@inproceedings{tang_line_2015,
	title = {Line: {Large}-scale information network embedding},
	shorttitle = {Line},
	booktitle = {Proceedings of the 24th international conference on world wide web},
	author = {Tang, Jian and Qu, Meng and Wang, Mingzhe and Zhang, Ming and Yan, Jun and Mei, Qiaozhu},
	year = {2015},
	pages = {1067--1077},
}

@inproceedings{perozzi_deepwalk_2014,
	title = {Deepwalk: {Online} learning of social representations},
	shorttitle = {Deepwalk},
	booktitle = {Proceedings of the 20th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	author = {Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
	year = {2014},
	pages = {701--710},
}

@inproceedings{grover_node2vec_2016,
	title = {node2vec: {Scalable} feature learning for networks},
	shorttitle = {node2vec},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	author = {Grover, Aditya and Leskovec, Jure},
	year = {2016},
	pages = {855--864},
}

@article{hu_open_2021,
	title = {Open {Graph} {Benchmark}: {Datasets} for {Machine} {Learning} on {Graphs}},
	shorttitle = {Open {Graph} {Benchmark}},
	url = {http://arxiv.org/abs/2005.00687},
	abstract = {We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale, encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at https://ogb.stanford.edu .},
	urldate = {2021-06-03},
	journal = {arXiv:2005.00687 [cs, stat]},
	author = {Hu, Weihua and Fey, Matthias and Zitnik, Marinka and Dong, Yuxiao and Ren, Hongyu and Liu, Bowen and Catasta, Michele and Leskovec, Jure},
	month = feb,
	year = {2021},
	note = {arXiv: 2005.00687},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Social and Information Networks},
}

@article{huang_combining_2020,
	title = {Combining {Label} {Propagation} and {Simple} {Models} {Out}-performs {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2010.13993},
	abstract = {Graph Neural Networks (GNNs) are the predominant technique for learning over graphs. However, there is relatively little understanding of why GNNs are successful in practice and whether they are necessary for good performance. Here, we show that for many standard transductive node classification benchmarks, we can exceed or match the performance of state-of-the-art GNNs by combining shallow models that ignore the graph structure with two simple post-processing steps that exploit correlation in the label structure: (i) an "error correlation" that spreads residual errors in training data to correct errors in test data and (ii) a "prediction correlation" that smooths the predictions on the test data. We call this overall procedure Correct and Smooth (C\&S), and the post-processing steps are implemented via simple modifications to standard label propagation techniques from early graph-based semi-supervised learning methods. Our approach exceeds or nearly matches the performance of state-of-the-art GNNs on a wide variety of benchmarks, with just a small fraction of the parameters and orders of magnitude faster runtime. For instance, we exceed the best known GNN performance on the OGB-Products dataset with 137 times fewer parameters and greater than 100 times less training time. The performance of our methods highlights how directly incorporating label information into the learning algorithm (as was done in traditional techniques) yields easy and substantial performance gains. We can also incorporate our techniques into big GNN models, providing modest gains. Our code for the OGB results is at https://github.com/Chillee/CorrectAndSmooth.},
	urldate = {2021-06-04},
	journal = {arXiv:2010.13993 [cs]},
	author = {Huang, Qian and He, Horace and Singh, Abhay and Lim, Ser-Nam and Benson, Austin R.},
	month = nov,
	year = {2020},
	note = {arXiv: 2010.13993},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks},
}

@article{salha_keep_2019,
	title = {Keep {It} {Simple}: {Graph} {Autoencoders} {Without} {Graph} {Convolutional} {Networks}},
	shorttitle = {Keep {It} {Simple}},
	url = {http://arxiv.org/abs/1910.00942},
	abstract = {Graph autoencoders (AE) and variational autoencoders (VAE) recently emerged as powerful node embedding methods, with promising performances on challenging tasks such as link prediction and node clustering. Graph AE, VAE and most of their extensions rely on graph convolutional networks (GCN) to learn vector space representations of nodes. In this paper, we propose to replace the GCN encoder by a simple linear model w.r.t. the adjacency matrix of the graph. For the two aforementioned tasks, we empirically show that this approach consistently reaches competitive performances w.r.t. GCN-based models for numerous real-world graphs, including the widely used Cora, Citeseer and Pubmed citation networks that became the de facto benchmark datasets for evaluating graph AE and VAE. This result questions the relevance of repeatedly using these three datasets to compare complex graph AE and VAE models. It also emphasizes the effectiveness of simple node encoding schemes for many real-world applications.},
	urldate = {2021-06-04},
	journal = {arXiv:1910.00942 [cs, stat]},
	author = {Salha, Guillaume and Hennequin, Romain and Vazirgiannis, Michalis},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.00942},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Social and Information Networks},
}

@article{frasca_sign_2020,
	title = {{SIGN}: {Scalable} {Inception} {Graph} {Neural} {Networks}},
	shorttitle = {{SIGN}},
	url = {http://arxiv.org/abs/2004.11198},
	abstract = {Graph representation learning has recently been applied to a broad spectrum of problems ranging from computer graphics and chemistry to high energy physics and social media. The popularity of graph neural networks has sparked interest, both in academia and in industry, in developing methods that scale to very large graphs such as Facebook or Twitter social networks. In most of these approaches, the computational cost is alleviated by a sampling strategy retaining a subset of node neighbors or subgraphs at training time. In this paper we propose a new, efficient and scalable graph deep learning architecture which sidesteps the need for graph sampling by using graph convolutional filters of different size that are amenable to efficient precomputation, allowing extremely fast training and inference. Our architecture allows using different local graph operators (e.g. motif-induced adjacency matrices or Personalized Page Rank diffusion matrix) to best suit the task at hand. We conduct extensive experimental evaluation on various open benchmarks and show that our approach is competitive with other state-of-the-art architectures, while requiring a fraction of the training and inference time. Moreover, we obtain state-of-the-art results on ogbn-papers100M, the largest public graph dataset, with over 110 million nodes and 1.5 billion edges.},
	urldate = {2021-06-04},
	journal = {arXiv:2004.11198 [cs, stat]},
	author = {Frasca, Fabrizio and Rossi, Emanuele and Eynard, Davide and Chamberlain, Ben and Bronstein, Michael and Monti, Federico},
	month = nov,
	year = {2020},
	note = {arXiv: 2004.11198},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
}

@inproceedings{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {8024--8035},
}

@incollection{tang_pte_2015,
	address = {New York, NY, USA},
	title = {{PTE}: {Predictive} {Text} {Embedding} through {Large}-scale {Heterogeneous} {Text} {Networks}},
	isbn = {978-1-4503-3664-2},
	shorttitle = {{PTE}},
	url = {https://doi.org/10.1145/2783258.2783307},
	abstract = {Unsupervised text embedding methods, such as Skip-gram and Paragraph Vector, have been attracting increasing attention due to their simplicity, scalability, and effectiveness. However, comparing to sophisticated deep learning architectures such as convolutional neural networks, these methods usually yield inferior results when applied to particular machine learning tasks. One possible reason is that these text embedding methods learn the representation of text in a fully unsupervised way, without leveraging the labeled information available for the task. Although the low dimensional representations learned are applicable to many different tasks, they are not particularly tuned for any task. In this paper, we fill this gap by proposing a semi-supervised representation learning method for text data, which we call the predictive text embedding (PTE). Predictive text embedding utilizes both labeled and unlabeled data to learn the embedding of text. The labeled information and different levels of word co-occurrence information are first represented as a large-scale heterogeneous text network, which is then embedded into a low dimensional space through a principled and efficient algorithm. This low dimensional embedding not only preserves the semantic closeness of words and documents, but also has a strong predictive power for the particular task. Compared to recent supervised approaches based on convolutional neural networks, predictive text embedding is comparable or more effective, much more efficient, and has fewer parameters to tune.},
	urldate = {2021-06-30},
	booktitle = {Proceedings of the 21th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Tang, Jian and Qu, Meng and Mei, Qiaozhu},
	month = aug,
	year = {2015},
	keywords = {predictive text embedding, representation learning},
	pages = {1165--1174},
}

@inproceedings{topping_understanding_2021,
	title = {Understanding over-squashing and bottlenecks on graphs via curvature},
	url = {https://openreview.net/forum?id=7UmjRGzp-A},
	abstract = {Most graph neural networks (GNNs) use the message passing paradigm, in which node features are propagated on the input graph. Recent works pointed to the distortion of information flowing from...},
	language = {en},
	urldate = {2022-06-12},
	author = {Topping, Jake and Giovanni, Francesco Di and Chamberlain, Benjamin Paul and Dong, Xiaowen and Bronstein, Michael M.},
	month = sep,
	year = {2021},
}

@misc{velickovic_geometric_2021,
	title = {Geometric {Deep} {Learning} - {Grids}, {Groups}, {Graphs}, {Geodesics}, and {Gauges}},
	url = {https://geometricdeeplearning.com/geometricdeeplearning.com/},
	abstract = {Grids, Groups, Graphs, Geodesics, and Gauges},
	urldate = {2022-06-12},
	author = {Veličković, Petar},
	year = {2021},
}

@inproceedings{yang_revisiting_2016,
	title = {Revisiting {Semi}-{Supervised} {Learning} with {Graph} {Embeddings}},
	url = {https://proceedings.mlr.press/v48/yanga16.html},
	abstract = {We present a semi-supervised learning framework based on graph embeddings. Given a graph between instances, we train an embedding for each instance to jointly predict the class label and the neighborhood context in the graph. We develop both transductive and inductive variants of our method. In the transductive variant of our method, the class labels are determined by both the learned embeddings and input feature vectors, while in the inductive variant, the embeddings are defined as a parametric function of the feature vectors, so predictions can be made on instances not seen during training. On a large and diverse set of benchmark tasks, including text classification, distantly supervised entity extraction, and entity classification, we show improved performance over many of the existing models.},
	language = {en},
	urldate = {2022-06-16},
	booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yang, Zhilin and Cohen, William and Salakhudinov, Ruslan},
	month = jun,
	year = {2016},
	note = {ISSN: 1938-7228},
	pages = {40--48},
}

@inproceedings{chen_fastgcn_2018,
	title = {{FastGCN}: {Fast} {Learning} with {Graph} {Convolutional} {Networks} via {Importance} {Sampling}},
	shorttitle = {{FastGCN}},
	url = {https://openreview.net/forum?id=rytstxWAW},
	abstract = {The graph convolutional networks (GCN) recently proposed by Kipf and Welling are an effective graph model for semi-supervised learning. Such a model, however, is transductive in nature because...},
	language = {en},
	urldate = {2022-06-16},
	author = {Chen, Jie and Ma, Tengfei and Xiao, Cao},
	month = feb,
	year = {2018},
}

@inproceedings{bojchevski_deep_2018,
	title = {Deep {Gaussian} {Embedding} of {Graphs}: {Unsupervised} {Inductive} {Learning} via {Ranking}},
	shorttitle = {Deep {Gaussian} {Embedding} of {Graphs}},
	url = {https://openreview.net/forum?id=r1ZdKJ-0W},
	abstract = {We embed nodes in a graph as Gaussian distributions allowing us to capture uncertainty about their representation.},
	language = {en},
	urldate = {2022-06-16},
	author = {Bojchevski, Aleksandar and Günnemann, Stephan},
	month = feb,
	year = {2018},
}

@article{srivastava_dropout_2014,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overfitting}},
	volume = {15},
	issn = {1533-7928},
	shorttitle = {Dropout},
	url = {http://jmlr.org/papers/v15/srivastava14a.html},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different âthinnedâ networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	number = {56},
	urldate = {2022-06-16},
	journal = {Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	pages = {1929--1958},
}

@misc{fey_fast_2019,
	title = {Fast {Graph} {Representation} {Learning} with {PyTorch} {Geometric}},
	url = {https://github.com/pyg-team/pytorch_geometric},
	author = {Fey, Matthias and Lenssen, Jan Eric},
	month = may,
	year = {2019},
}

@article{kipf_semi-supervised_2016,
	title = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
	url = {https://openreview.net/forum?id=SJU4ayYgl},
	abstract = {Semi-supervised classification with a CNN model for graphs. State-of-the-art results on a number of citation network datasets.},
	language = {en},
	urldate = {2022-06-17},
	author = {Kipf, Thomas N. and Welling, Max},
	month = nov,
	year = {2016},
}

@inproceedings{defferrard_convolutional_2016,
	title = {Convolutional {Neural} {Networks} on {Graphs} with {Fast} {Localized} {Spectral} {Filtering}},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/hash/04df4d434d481c5bb723be1b6df1ee65-Abstract.html},
	abstract = {In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words’ embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
	urldate = {2022-06-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Defferrard, Michaël and Bresson, Xavier and Vandergheynst, Pierre},
	year = {2016},
}

@article{li_deepergcn_2021,
	title = {{DeeperGCN}: {All} {You} {Need} to {Train} {Deeper} {GCNs}},
	shorttitle = {{DeeperGCN}},
	url = {https://openreview.net/forum?id=qOcf6HgSmRH},
	abstract = {Graph Neural Networks (GNNs) have been drawing significant attention to the power of representation learning on graphs. Recent works developed frameworks to train very deep GNNs. Such works show...},
	language = {en},
	urldate = {2022-06-17},
	author = {Li, Guohao and Xiong, Chenxin and Qian, Guocheng and Thabet, Ali and Ghanem, Bernard},
	month = sep,
	year = {2021},
}

@inproceedings{chamberlain_grand_2021,
	title = {{GRAND}: {Graph} {Neural} {Diffusion}},
	shorttitle = {{GRAND}},
	url = {https://openreview.net/forum?id=_1fu_cjsaRE},
	abstract = {We present Graph Neural Diffusion (GRAND), a model that approaches deep learning on graphs as a continuous diffusion process and treats Graph Neural Networks (GNNs) as discretisations of an...},
	language = {en},
	urldate = {2022-06-17},
	author = {Chamberlain, Benjamin Paul and Rowbottom, James and Gorinova, Maria I. and Webb, Stefan D. and Rossi, Emanuele and Bronstein, Michael M.},
	month = sep,
	year = {2021},
}

@article{zhang_eigen-gnn_2021,
	title = {Eigen-{GNN}: a {Graph} {Structure} {Preserving} {Plug}-in for {GNNs}},
	issn = {1558-2191},
	shorttitle = {Eigen-{GNN}},
	doi = {10.1109/TKDE.2021.3112746},
	abstract = {Graph Neural Networks (GNNs) are emerging machine learning models on graphs. Although sufficiently deep GNNs are shown theoretically capable of fully preserving graph structures, most existing GNN models in practice are shallow and essentially feature-centric. We show empirically and analytically that the existing shallow GNNs cannot preserve graph structures well. To overcome this fundamental challenge, we propose Eigen-GNN, a simple yet effective and general plug-in module to boost GNNs ability in preserving graph structures. Specifically, we integrate the eigenspace of graph structures with GNNs by treating GNNs as a type of dimensionality reduction and expanding the initial dimensionality reduction bases. Without needing to increase depths, Eigen-GNN possesses more flexibilities in handling both feature-driven and structure-driven tasks since the initial bases contain both node features and graph structures. We present extensive experimental results to demonstrate the effectiveness of Eigen-GNN for tasks including node classification, link prediction, and graph isomorphism tests.},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhang, Ziwei and Cui, Peng and Pei, Jian and Wang, Xin and Zhu, Wenwu},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Convolution, Convolutional codes, Dimensionality Reduction, Eigenvector, Graph Neural Networks, Graph Structure, Laplace equations, Smoothing methods, Social networking (online), Task analysis, Training},
	pages = {1--1},
}

@inproceedings{gasteiger_diffusion_2019,
	title = {Diffusion {Improves} {Graph} {Learning}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/23c894276a2c5a16470e6a31f4618d73-Abstract.html},
	abstract = {Graph convolution is the core of most Graph Neural Networks (GNNs) and usually approximated by message passing between direct (one-hop) neighbors. In this work, we remove the restriction of using only the direct neighbors by introducing a powerful, yet spatially localized graph convolution: Graph diffusion convolution (GDC). GDC leverages generalized graph diffusion, examples of which are the heat kernel and personalized PageRank. It alleviates the problem of noisy and often arbitrarily defined edges in real graphs. We show that GDC is closely related to spectral-based models and thus combines the strengths of both spatial (message passing) and spectral methods. We demonstrate that replacing message passing with graph diffusion convolution consistently leads to significant performance improvements across a wide range of models on both supervised and unsupervised tasks and a variety of datasets. Furthermore, GDC is not limited to GNNs but can trivially be combined with any graph-based model or algorithm (e.g. spectral clustering) without requiring any changes to the latter or affecting its computational complexity. Our implementation is available online.},
	urldate = {2022-06-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Gasteiger, Johannes and Weiß enberger, Stefan and Günnemann, Stephan},
	year = {2019},
}

@misc{page_pagerank_1999,
	type = {Techreport},
	title = {The {PageRank} {Citation} {Ranking}: {Bringing} {Order} to the {Web}.},
	shorttitle = {The {PageRank} {Citation} {Ranking}},
	url = {http://ilpubs.stanford.edu:8090/422/},
	abstract = {The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a mathod for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.},
	urldate = {2022-06-18},
	author = {Page, Lawrence and Brin, Sergey and Motwani, Rajeev and Winograd, Terry},
	month = nov,
	year = {1999},
	note = {Publisher: Stanford InfoLab},
}

@inproceedings{kondor_diffusion_2002,
	title = {Diffusion kernels on graphs and other discrete structures},
	volume = {2002},
	booktitle = {Proceedings of the 19th international conference on machine learning},
	author = {Kondor, Risi Imre and Lafferty, John},
	year = {2002},
	pages = {315--322},
}

@inproceedings{morris_tudataset_2020,
	title = {{TUDataset}: {A} collection of benchmark datasets for learning with graphs},
	url = {www.graphlearning.io},
	booktitle = {{ICML} 2020 {Workshop} on {Graph} {Representation} {Learning} and {Beyond} ({GRL}+ 2020)},
	author = {Morris, Christopher and Kriege, Nils M. and Bause, Franka and Kersting, Kristian and Mutzel, Petra and Neumann, Marion},
	year = {2020},
	note = {\_eprint: 2007.08663},
}

@techreport{dedic_graph_2021,
	title = {Graph {Coarsening} {Can} {Increase} {Learning} {Efficiency}},
	copyright = {All rights reserved},
	url = {http://kmwww.fjfi.cvut.cz/ddny},
	abstract = {Graph based models are used for tasks with increasing size and computational demands. The paper
focuses on leveraging methods for pretraining on coarser graphs with HARP as the method of choice. The
method is generalized using partially injective homomorphisms, a concept from the field of data mining.
Such a way of producing graph coarsenings is shown to be feasible and not to affect the performance of
HARP in a negative way. Also, the performance-complexity characterics of these methods are studied
and HARP is established as a way of efficient pretraining which can reduce the ammount of computational
power needed to train graph-based models on large data.},
	institution = {Czech Technical University in Prague},
	author = {Dědič, Marek},
	month = nov,
	year = {2021},
	pages = {13--21},
}
