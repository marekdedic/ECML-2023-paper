
@inproceedings{kipf_semi-supervised_2017,
	address = {Toulon, France},
	title = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
	url = {https://openreview.net/forum?id=SJU4ayYgl},
	abstract = {Semi-supervised classification with a CNN model for graphs. State-of-the-art results on a number of citation network datasets.},
	language = {en},
	booktitle = {5th {International} {Conference} on {Learning} {Representations}, \{{ICLR}\} 2017, {Toulon}, {France}, {April} 24-26, 2017, {Conference} {Track} {Proceedings}},
	publisher = {OpenReview.net},
	author = {Kipf, Thomas N. and Welling, Max},
	month = apr,
	year = {2017},
}

@inproceedings{grover_node2vec_2016,
	title = {node2vec: {Scalable} feature learning for networks},
	shorttitle = {node2vec},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	author = {Grover, Aditya and Leskovec, Jure},
	year = {2016},
	pages = {855--864},
	file = {Full Text:/home/cisco/Zotero/storage/Y8U8YZQI/PMC5108654.html:text/html;Snapshot:/home/cisco/Zotero/storage/ITX5DJ45/2939672.html:text/html},
}

@inproceedings{perozzi_deepwalk_2014,
	title = {Deepwalk: {Online} learning of social representations},
	shorttitle = {Deepwalk},
	booktitle = {Proceedings of the 20th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	author = {Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
	year = {2014},
	pages = {701--710},
	file = {Full Text:/home/cisco/Zotero/storage/ISN68YG2/Perozzi et al. - 2014 - Deepwalk Online learning of social representation.pdf:application/pdf;Snapshot:/home/cisco/Zotero/storage/Y3TWLT7L/2623330.html:text/html},
}

@misc{kingma_adam:_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2020-06-25},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980},
	keywords = {Computer Science - Learning, Computer Science - Machine Learning},
}

@article{chen_harp_2018,
	title = {{HARP}: {Hierarchical} {Representation} {Learning} for {Networks}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	shorttitle = {{HARP}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11849},
	language = {en},
	number = {1},
	urldate = {2021-05-25},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chen, Haochen and Perozzi, Bryan and Hu, Yifan and Skiena, Steven},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {network classification},
}

@inproceedings{schulz_mining_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Mining {Tree} {Patterns} with {Partially} {Injective} {Homomorphisms}},
	isbn = {978-3-030-10928-8},
	doi = {10.1007/978-3-030-10928-8_35},
	abstract = {One of the main differences between inductive logic programming (ILP) and graph mining lies in the pattern matching operator applied: While it is mainly defined by relational homomorphism (i.e., subsumption) in ILP, subgraph isomorphism is the most common pattern matching operator in graph mining. Using the fact that subgraph isomorphisms are injective homomorphisms, we bridge the gap between ILP and graph mining by considering a natural transition from homomorphisms to subgraph isomorphisms that is defined by partially injective homomorphisms, i.e., which require injectivity only for subsets of the vertex pairs in the pattern. Utilizing positive complexity results on deciding homomorphisms from bounded tree-width graphs, we present an algorithm mining frequent trees from arbitrary graphs w.r.t. partially injective homomorphisms. Our experimental results show that the predictive performance of the patterns obtained is comparable to that of ordinary frequent subgraphs. Thus, by preserving much from the advantageous properties of homomorphisms and subgraph isomorphisms, our approach provides a trade-off between efficiency and predictive power.},
	language = {en},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer International Publishing},
	author = {Schulz, Till Hendrik and Horv{\'a}th, Tam{\'a}s and Welke, Pascal and Wrobel, Stefan},
	editor = {Berlingerio, Michele and Bonchi, Francesco and G{\"a}rtner, Thomas and Hurley, Neil and Ifrim, Georgiana},
	year = {2019},
	pages = {585--601},
}

@inproceedings{tang_line_2015,
	title = {Line: {Large}-scale information network embedding},
	shorttitle = {Line},
	booktitle = {Proceedings of the 24th international conference on world wide web},
	author = {Tang, Jian and Qu, Meng and Wang, Mingzhe and Zhang, Ming and Yan, Jun and Mei, Qiaozhu},
	year = {2015},
	pages = {1067--1077},
}

@misc{hu_open_2021,
	title = {Open {Graph} {Benchmark}: {Datasets} for {Machine} {Learning} on {Graphs}},
	shorttitle = {Open {Graph} {Benchmark}},
	abstract = {We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale, encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at https://ogb.stanford.edu .},
	urldate = {2021-06-03},
	publisher = {arXiv},
	author = {Hu, Weihua and Fey, Matthias and Zitnik, Marinka and Dong, Yuxiao and Ren, Hongyu and Liu, Bowen and Catasta, Michele and Leskovec, Jure},
	month = feb,
	year = {2021},
	note = {arXiv: 2005.00687},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Social and Information Networks},
}

@misc{huang_combining_2020,
	title = {Combining {Label} {Propagation} and {Simple} {Models} {Out}-performs {Graph} {Neural} {Networks}},
	abstract = {Graph Neural Networks (GNNs) are the predominant technique for learning over graphs. However, there is relatively little understanding of why GNNs are successful in practice and whether they are necessary for good performance. Here, we show that for many standard transductive node classification benchmarks, we can exceed or match the performance of state-of-the-art GNNs by combining shallow models that ignore the graph structure with two simple post-processing steps that exploit correlation in the label structure: (i) an "error correlation" that spreads residual errors in training data to correct errors in test data and (ii) a "prediction correlation" that smooths the predictions on the test data. We call this overall procedure Correct and Smooth (C\&S), and the post-processing steps are implemented via simple modifications to standard label propagation techniques from early graph-based semi-supervised learning methods. Our approach exceeds or nearly matches the performance of state-of-the-art GNNs on a wide variety of benchmarks, with just a small fraction of the parameters and orders of magnitude faster runtime. For instance, we exceed the best known GNN performance on the OGB-Products dataset with 137 times fewer parameters and greater than 100 times less training time. The performance of our methods highlights how directly incorporating label information into the learning algorithm (as was done in traditional techniques) yields easy and substantial performance gains. We can also incorporate our techniques into big GNN models, providing modest gains. Our code for the OGB results is at https://github.com/Chillee/CorrectAndSmooth.},
	urldate = {2021-06-04},
	publisher = {arXiv},
	author = {Huang, Qian and He, Horace and Singh, Abhay and Lim, Ser-Nam and Benson, Austin R.},
	month = nov,
	year = {2020},
	note = {arXiv:2010.13993},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks},
}

@article{salha_keep_2019,
	title = {Keep {It} {Simple}: {Graph} {Autoencoders} {Without} {Graph} {Convolutional} {Networks}},
	shorttitle = {Keep {It} {Simple}},
	url = {http://arxiv.org/abs/1910.00942},
	abstract = {Graph autoencoders (AE) and variational autoencoders (VAE) recently emerged as powerful node embedding methods, with promising performances on challenging tasks such as link prediction and node clustering. Graph AE, VAE and most of their extensions rely on graph convolutional networks (GCN) to learn vector space representations of nodes. In this paper, we propose to replace the GCN encoder by a simple linear model w.r.t. the adjacency matrix of the graph. For the two aforementioned tasks, we empirically show that this approach consistently reaches competitive performances w.r.t. GCN-based models for numerous real-world graphs, including the widely used Cora, Citeseer and Pubmed citation networks that became the de facto benchmark datasets for evaluating graph AE and VAE. This result questions the relevance of repeatedly using these three datasets to compare complex graph AE and VAE models. It also emphasizes the effectiveness of simple node encoding schemes for many real-world applications.},
	urldate = {2021-06-04},
	journal = {arXiv:1910.00942 [cs, stat]},
	author = {Salha, Guillaume and Hennequin, Romain and Vazirgiannis, Michalis},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.00942},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Social and Information Networks},
}

@misc{frasca_sign_2020,
	title = {{SIGN}: {Scalable} {Inception} {Graph} {Neural} {Networks}},
	shorttitle = {{SIGN}},
	abstract = {Graph representation learning has recently been applied to a broad spectrum of problems ranging from computer graphics and chemistry to high energy physics and social media. The popularity of graph neural networks has sparked interest, both in academia and in industry, in developing methods that scale to very large graphs such as Facebook or Twitter social networks. In most of these approaches, the computational cost is alleviated by a sampling strategy retaining a subset of node neighbors or subgraphs at training time. In this paper we propose a new, efficient and scalable graph deep learning architecture which sidesteps the need for graph sampling by using graph convolutional filters of different size that are amenable to efficient precomputation, allowing extremely fast training and inference. Our architecture allows using different local graph operators (e.g. motif-induced adjacency matrices or Personalized Page Rank diffusion matrix) to best suit the task at hand. We conduct extensive experimental evaluation on various open benchmarks and show that our approach is competitive with other state-of-the-art architectures, while requiring a fraction of the training and inference time. Moreover, we obtain state-of-the-art results on ogbn-papers100M, the largest public graph dataset, with over 110 million nodes and 1.5 billion edges.},
	urldate = {2021-06-04},
	publisher = {arXiv},
	author = {Frasca, Fabrizio and Rossi, Emanuele and Eynard, Davide and Chamberlain, Ben and Bronstein, Michael and Monti, Federico},
	month = nov,
	year = {2020},
	note = {arXiv: 2004.11198},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
}

@inproceedings{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alch{\'e}-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {8024--8035},
}

@misc{fey_fast_2019,
	title = {Fast {Graph} {Representation} {Learning} with {PyTorch} {Geometric}},
	abstract = {We introduce PyTorch Geometric, a library for deep learning on irregularly structured input data such as graphs, point clouds and manifolds, built upon PyTorch. In addition to general graph data structures and processing methods, it contains a variety of recently published methods from the domains of relational learning and 3D data processing. PyTorch Geometric achieves high data throughput by leveraging sparse GPU acceleration, by providing dedicated CUDA kernels and by introducing efficient mini-batch handling for input examples of different size. In this work, we present the library in detail and perform a comprehensive comparative study of the implemented methods in homogeneous evaluation scenarios.},
	urldate = {2021-06-22},
	publisher = {arXiv},
	author = {Fey, Matthias and Lenssen, Jan Eric},
	month = apr,
	year = {2019},
	note = {arXiv: 1903.02428},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
}

@incollection{tang_pte_2015,
	address = {New York, NY, USA},
	title = {{PTE}: {Predictive} {Text} {Embedding} through {Large}-scale {Heterogeneous} {Text} {Networks}},
	isbn = {978-1-4503-3664-2},
	shorttitle = {{PTE}},
	url = {https://doi.org/10.1145/2783258.2783307},
	abstract = {Unsupervised text embedding methods, such as Skip-gram and Paragraph Vector, have been attracting increasing attention due to their simplicity, scalability, and effectiveness. However, comparing to sophisticated deep learning architectures such as convolutional neural networks, these methods usually yield inferior results when applied to particular machine learning tasks. One possible reason is that these text embedding methods learn the representation of text in a fully unsupervised way, without leveraging the labeled information available for the task. Although the low dimensional representations learned are applicable to many different tasks, they are not particularly tuned for any task. In this paper, we fill this gap by proposing a semi-supervised representation learning method for text data, which we call the predictive text embedding (PTE). Predictive text embedding utilizes both labeled and unlabeled data to learn the embedding of text. The labeled information and different levels of word co-occurrence information are first represented as a large-scale heterogeneous text network, which is then embedded into a low dimensional space through a principled and efficient algorithm. This low dimensional embedding not only preserves the semantic closeness of words and documents, but also has a strong predictive power for the particular task. Compared to recent supervised approaches based on convolutional neural networks, predictive text embedding is comparable or more effective, much more efficient, and has fewer parameters to tune.},
	urldate = {2021-06-30},
	booktitle = {Proceedings of the 21th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Tang, Jian and Qu, Meng and Mei, Qiaozhu},
	month = aug,
	year = {2015},
	keywords = {predictive text embedding, representation learning},
	pages = {1165--1174},
}

@inproceedings{topping_understanding_2021,
	title = {Understanding over-squashing and bottlenecks on graphs via curvature},
	url = {https://openreview.net/forum?id=7UmjRGzp-A},
	abstract = {Most graph neural networks (GNNs) use the message passing paradigm, in which node features are propagated on the input graph. Recent works pointed to the distortion of information flowing from...},
	language = {en},
	urldate = {2022-06-12},
	booktitle = {The {Tenth} {International} {Conference} on {Learning} {Representations}},
	author = {Topping, Jake and Giovanni, Francesco Di and Chamberlain, Benjamin Paul and Dong, Xiaowen and Bronstein, Michael M.},
	month = sep,
	year = {2021},
}

@misc{velickovic_geometric_2021,
	title = {Geometric {Deep} {Learning} - {Grids}, {Groups}, {Graphs}, {Geodesics}, and {Gauges}},
	url = {https://geometricdeeplearning.com/geometricdeeplearning.com/},
	abstract = {Grids, Groups, Graphs, Geodesics, and Gauges},
	urldate = {2022-06-12},
	author = {Veli{\v c}kovi{\'c}, Petar},
	year = {2021},
}

@inproceedings{yang_revisiting_2016,
	title = {Revisiting {Semi}-{Supervised} {Learning} with {Graph} {Embeddings}},
	url = {https://proceedings.mlr.press/v48/yanga16.html},
	abstract = {We present a semi-supervised learning framework based on graph embeddings. Given a graph between instances, we train an embedding for each instance to jointly predict the class label and the neighborhood context in the graph. We develop both transductive and inductive variants of our method. In the transductive variant of our method, the class labels are determined by both the learned embeddings and input feature vectors, while in the inductive variant, the embeddings are defined as a parametric function of the feature vectors, so predictions can be made on instances not seen during training. On a large and diverse set of benchmark tasks, including text classification, distantly supervised entity extraction, and entity classification, we show improved performance over many of the existing models.},
	language = {en},
	urldate = {2022-06-16},
	booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yang, Zhilin and Cohen, William and Salakhudinov, Ruslan},
	month = jun,
	year = {2016},
	pages = {40--48},
}

@inproceedings{chen_fastgcn_2018,
	title = {{FastGCN}: {Fast} {Learning} with {Graph} {Convolutional} {Networks} via {Importance} {Sampling}},
	shorttitle = {{FastGCN}},
	url = {https://openreview.net/forum?id=rytstxWAW},
	abstract = {The graph convolutional networks (GCN) recently proposed by Kipf and Welling are an effective graph model for semi-supervised learning. Such a model, however, is transductive in nature because...},
	language = {en},
	urldate = {2022-06-16},
	booktitle = {6th {International} {Conference} on {Learning} {Representations}},
	author = {Chen, Jie and Ma, Tengfei and Xiao, Cao},
	month = feb,
	year = {2018},
}

@inproceedings{bojchevski_deep_2018,
	title = {Deep {Gaussian} {Embedding} of {Graphs}: {Unsupervised} {Inductive} {Learning} via {Ranking}},
	shorttitle = {Deep {Gaussian} {Embedding} of {Graphs}},
	url = {https://openreview.net/forum?id=r1ZdKJ-0W},
	abstract = {We embed nodes in a graph as Gaussian distributions allowing us to capture uncertainty about their representation.},
	language = {en},
	urldate = {2022-06-16},
	booktitle = {6th {International} {Conference} on {Learning} {Representations}},
	author = {Bojchevski, Aleksandar and G{\"u}nnemann, Stephan},
	month = feb,
	year = {2018},
}

@article{srivastava_dropout_2014,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overfitting}},
	volume = {15},
	issn = {1533-7928},
	shorttitle = {Dropout},
	url = {http://jmlr.org/papers/v15/srivastava14a.html},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different {\^a}??thinned{\^a}?? networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	number = {56},
	urldate = {2022-06-16},
	journal = {Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	pages = {1929--1958},
}

@inproceedings{defferrard_convolutional_2016,
	title = {Convolutional {Neural} {Networks} on {Graphs} with {Fast} {Localized} {Spectral} {Filtering}},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/hash/04df4d434d481c5bb723be1b6df1ee65-Abstract.html},
	abstract = {In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words{\textquoteright} embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
	urldate = {2022-06-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Defferrard, Micha{\"e}l and Bresson, Xavier and Vandergheynst, Pierre},
	year = {2016},
}

@inproceedings{chamberlain_grand_2021,
	title = {{GRAND}: {Graph} {Neural} {Diffusion}},
	shorttitle = {{GRAND}},
	url = {https://openreview.net/forum?id=_1fu_cjsaRE},
	abstract = {We present Graph Neural Diffusion (GRAND), a model that approaches deep learning on graphs as a continuous diffusion process and treats Graph Neural Networks (GNNs) as discretisations of an...},
	language = {en},
	urldate = {2022-06-17},
	booktitle = {The {Symbiosis} of {Deep} {Learning} and {Differential} {Equations}},
	author = {Chamberlain, Benjamin Paul and Rowbottom, James and Gorinova, Maria I. and Webb, Stefan D. and Rossi, Emanuele and Bronstein, Michael M.},
	month = sep,
	year = {2021},
}

@article{zhang_eigen-gnn_2021,
	title = {Eigen-{GNN}: a {Graph} {Structure} {Preserving} {Plug}-in for {GNNs}},
	issn = {1558-2191},
	shorttitle = {Eigen-{GNN}},
	doi = {10.1109/TKDE.2021.3112746},
	abstract = {Graph Neural Networks (GNNs) are emerging machine learning models on graphs. Although sufficiently deep GNNs are shown theoretically capable of fully preserving graph structures, most existing GNN models in practice are shallow and essentially feature-centric. We show empirically and analytically that the existing shallow GNNs cannot preserve graph structures well. To overcome this fundamental challenge, we propose Eigen-GNN, a simple yet effective and general plug-in module to boost GNNs ability in preserving graph structures. Specifically, we integrate the eigenspace of graph structures with GNNs by treating GNNs as a type of dimensionality reduction and expanding the initial dimensionality reduction bases. Without needing to increase depths, Eigen-GNN possesses more flexibilities in handling both feature-driven and structure-driven tasks since the initial bases contain both node features and graph structures. We present extensive experimental results to demonstrate the effectiveness of Eigen-GNN for tasks including node classification, link prediction, and graph isomorphism tests.},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhang, Ziwei and Cui, Peng and Pei, Jian and Wang, Xin and Zhu, Wenwu},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Convolution, Convolutional codes, Dimensionality Reduction, Eigenvector, Graph Neural Networks, Graph Structure, Laplace equations, Smoothing methods, Social networking (online), Task analysis, Training},
	pages = {1--1},
}

@inproceedings{gasteiger_diffusion_2019,
	title = {Diffusion {Improves} {Graph} {Learning}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/23c894276a2c5a16470e6a31f4618d73-Abstract.html},
	abstract = {Graph convolution is the core of most Graph Neural Networks (GNNs) and usually approximated by message passing between direct (one-hop) neighbors. In this work, we remove the restriction of using only the direct neighbors by introducing a powerful, yet spatially localized graph convolution: Graph diffusion convolution (GDC). GDC leverages generalized graph diffusion, examples of which are the heat kernel and personalized PageRank. It alleviates the problem of noisy and often arbitrarily defined edges in real graphs. We show that GDC is closely related to spectral-based models and thus combines the strengths of both spatial (message passing) and spectral methods. We demonstrate that replacing message passing with graph diffusion convolution consistently leads to significant performance improvements across a wide range of models on both supervised and unsupervised tasks and a variety of datasets. Furthermore, GDC is not limited to GNNs but can trivially be combined with any graph-based model or algorithm (e.g. spectral clustering) without requiring any changes to the latter or affecting its computational complexity. Our implementation is available online.},
	urldate = {2022-06-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Gasteiger, Johannes and Wei{\ss} enberger, Stefan and G{\"u}nnemann, Stephan},
	year = {2019},
}

@misc{page_pagerank_1999,
	type = {Techreport},
	title = {The {PageRank} {Citation} {Ranking}: {Bringing} {Order} to the {Web}.},
	shorttitle = {The {PageRank} {Citation} {Ranking}},
	url = {http://ilpubs.stanford.edu:8090/422/},
	abstract = {The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a mathod for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.},
	urldate = {2022-06-18},
	author = {Page, Lawrence and Brin, Sergey and Motwani, Rajeev and Winograd, Terry},
	month = nov,
	year = {1999},
	note = {Publisher: Stanford InfoLab},
}

@inproceedings{kondor_diffusion_2002,
	title = {Diffusion kernels on graphs and other discrete structures},
	volume = {2002},
	booktitle = {Proceedings of the 19th international conference on machine learning},
	author = {Kondor, Risi Imre and Lafferty, John},
	year = {2002},
	pages = {315--322},
}

@inproceedings{morris_tudataset_2020,
	title = {{TUDataset}: {A} collection of benchmark datasets for learning with graphs},
	url = {www.graphlearning.io},
	booktitle = {{ICML} 2020 {Workshop} on {Graph} {Representation} {Learning} and {Beyond} ({GRL}+ 2020)},
	author = {Morris, Christopher and Kriege, Nils M. and Bause, Franka and Kersting, Kristian and Mutzel, Petra and Neumann, Marion},
	year = {2020},
}

@techreport{dedic_graph_2021,
	title = {Graph {Coarsening} {Can} {Increase} {Learning} {Efficiency}},
	copyright = {All rights reserved},
	url = {http://kmwww.fjfi.cvut.cz/ddny},
	abstract = {Graph based models are used for tasks with increasing size and computational demands. The paper
focuses on leveraging methods for pretraining on coarser graphs with HARP as the method of choice. The
method is generalized using partially injective homomorphisms, a concept from the field of data mining.
Such a way of producing graph coarsenings is shown to be feasible and not to affect the performance of
HARP in a negative way. Also, the performance-complexity characterics of these methods are studied
and HARP is established as a way of efficient pretraining which can reduce the ammount of computational
power needed to train graph-based models on large data.},
	institution = {Czech Technical University in Prague},
	author = {D{\v e}di{\v c}, Marek},
	month = nov,
	year = {2021},
	pages = {13--21},
}

@inproceedings{akyildiz_understanding_2020,
	title = {Understanding {Coarsening} for {Embedding} {Large}-{Scale} {Graphs}},
	doi = {10.1109/BigData50022.2020.9377898},
	abstract = {A significant portion of the data today, e.g, social networks, web connections, etc., can be modeled by graphs. A proper analysis of graphs with Machine Learning (ML) algorithms has the potential to yield far-reaching insights into many areas of research and industry. However, the irregular structure of graph data constitutes an obstacle for running ML tasks on graphs such as link prediction, node classification, and anomaly detection. Graph embedding is a compute-intensive process of representing graphs as a set of vectors in a d-dimensional space, which in turn makes it amenable to ML tasks. Many approaches have been proposed in the literature to improve the performance of graph embedding, e.g., using distributed algorithms, accelerators, and pre-processing techniques. Graph coarsening, which can be considered a pre-processing step, is a structural approximation of a given, large graph with a smaller one. As the literature suggests, the cost of embedding significantly decreases when coarsening is employed. In this work, we thoroughly analyze the impact of the coarsening quality on the embedding performance both in terms of speed and accuracy. Our experiments with a state-of-the-art, fast graph embedding tool show that there is an interplay between the coarsening decisions taken and the embedding quality.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Akyildiz, Taha Atahan and Alabsi Aljundi, Amro and Kaya, Kamer},
	month = dec,
	year = {2020},
	keywords = {Machine learning, Machine learning algorithms, Social networking (online), Task analysis, Big Data, Graph coarsening, graph embedding, multi-level approach, Prediction algorithms, Tools},
	pages = {2937--2946},
}

@inproceedings{huang_scaling_2021,
	address = {New York, NY, USA},
	series = {{KDD} '21},
	title = {Scaling {Up} {Graph} {Neural} {Networks} {Via} {Graph} {Coarsening}},
	isbn = {978-1-4503-8332-5},
	doi = {10.1145/3447548.3467256},
	abstract = {Scalability of graph neural networks remains one of the major challenges in graph machine learning. Since the representation of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes from previous layers, the receptive fields grow exponentially, which makes standard stochastic optimization techniques ineffective. Various approaches have been proposed to alleviate this issue, e.g., sampling-based methods and techniques based on pre-computation of graph filters. In this paper, we take a different approach and propose to use graph coarsening for scalable training of GNNs, which is generic, extremely simple and has sublinear memory and time costs during training. We present extensive theoretical analysis on the effect of using coarsening operations and provides useful guidance on the choice of coarsening methods. Interestingly, our theoretical analysis shows that coarsening can also be considered as a type of regularization and may improve the generalization. Finally, empirical results on real world datasets show that, simply applying off-the-shelf coarsening methods, we can reduce the number of nodes by up to a factor of ten without causing a noticeable downgrade in classification accuracy.},
	urldate = {2022-06-20},
	booktitle = {Proceedings of the 27th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Huang, Zengfeng and Zhang, Shengzhong and Xi, Chong and Liu, Tang and Zhou, Min},
	month = aug,
	year = {2021},
	keywords = {graph coarsening, graph neural networks, scalable training},
	pages = {675--684},
}

@article{loukas_graph_2019,
	title = {Graph {Reduction} with {Spectral} and {Cut} {Guarantees}.},
	volume = {20},
	number = {116},
	journal = {J. Mach. Learn. Res.},
	author = {Loukas, Andreas},
	year = {2019},
	pages = {1--42},
}

@article{makarov_survey_2021,
	title = {Survey on graph embeddings and their applications to machine learning problems on graphs},
	volume = {7},
	issn = {2376-5992},
	url = {https://peerj.com/articles/cs-357},
	doi = {10.7717/peerj-cs.357},
	abstract = {Dealing with relational data always required significant computational resources, domain expertise and task-dependent feature engineering to incorporate structural information into a predictive model. Nowadays, a family of automated graph feature engineering techniques has been proposed in different streams of literature. So-called graph embeddings provide a powerful tool to construct vectorized feature spaces for graphs and their components, such as nodes, edges and subgraphs under preserving inner graph properties. Using the constructed feature spaces, many machine learning problems on graphs can be solved via standard frameworks suitable for vectorized feature representation. Our survey aims to describe the core concepts of graph embeddings and provide several taxonomies for their description. First, we start with the methodological approach and extract three types of graph embedding models based on matrix factorization, random-walks and deep learning approaches. Next, we describe how different types of networks impact the ability of models to incorporate structural and attributed data into a unified embedding. Going further, we perform a thorough evaluation of graph embedding applications to machine learning problems on graphs, among which are node classification, link prediction, clustering, visualization, compression, and a family of the whole graph embedding algorithms suitable for graph classification, similarity and alignment problems. Finally, we overview the existing applications of graph embeddings to computer science domains, formulate open problems and provide experiment results, explaining how different networks properties result in graph embeddings quality in the four classic machine learning problems on graphs, such as node classification, link prediction, clustering and graph visualization. As a result, our survey covers a new rapidly growing field of network feature engineering, presents an in-depth analysis of models based on network types, and overviews a wide range of applications to machine learning problems on graphs.},
	language = {en},
	urldate = {2022-06-21},
	journal = {PeerJ Computer Science},
	author = {Makarov, Ilya and Kiselev, Dmitrii and Nikitinsky, Nikita and Subelj, Lovro},
	month = feb,
	year = {2021},
	note = {Publisher: PeerJ Inc.},
	pages = {e357},
}

@article{chen_graph_2022,
	title = {Graph coarsening: from scientific computing to machine learning},
	volume = {79},
	issn = {2281-7875},
	shorttitle = {Graph coarsening},
	doi = {10.1007/s40324-021-00282-x},
	abstract = {The general method of graph coarsening or graph reduction has been a remarkably useful and ubiquitous tool in scientific computing and it is now just starting to have a similar impact in machine learning. The goal of this paper is to take a broad look into coarsening techniques that have been successfully deployed in scientific computing and see how similar principles are finding their way in more recent applications related to machine learning. In scientific computing, coarsening plays a central role in algebraic multigrid methods as well as the related class of multilevel incomplete LU factorizations. In machine learning, graph coarsening goes under various names, e.g., graph downsampling or graph reduction. Its goal in most cases is to replace some original graph by one which has fewer nodes, but whose structure and characteristics are similar to those of the original graph. As will be seen, a common strategy in these methods is to rely on spectral properties to define the coarse graph.},
	language = {en},
	number = {1},
	urldate = {2022-06-22},
	journal = {SeMA Journal},
	author = {Chen, Jie and Saad, Yousef and Zhang, Zechen},
	month = mar,
	year = {2022},
	keywords = {05C85, 65F10, 65N55m, 68T05, 94C15, Coarsening, Graph Coarsening, Graphs and Networks, Hierarchical methods. Graph Neural Networks, Multilevel methods},
	pages = {187--223},
}

@article{xie_graph_2020,
	title = {Graph convolutional networks with multi-level coarsening for graph classification},
	volume = {194},
	issn = {0950-7051},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120300629},
	doi = {10.1016/j.knosys.2020.105578},
	abstract = {Graph convolutional networks (GCNs) have attracted increasing attention in recent years. Many important tasks in graph analysis involve graph classification which aims to map a graph to a certain category. However, as the number of convolutional layers increases, most existing GCNs suffer from the problem of over-smoothing, which makes it difficult to extract the hierarchical information and global patterns of graphs when learning its representations. In this paper, we propose a multi-level coarsening based GCN (MLC-GCN) for graph classification. Specifically, from the perspective of graph analysis, we develop new insights into the convolutional architecture of image classification. Inspired by this, the two-stage MLC-GCN architecture is presented. In the architecture, we first introduce an adaptive structural coarsening module to produce a series of coarsened graphs and then construct the convolutional network based on these graphs. In contrast to existing GCNs, MLC-GCN has the advantages of learning graph representations at multiple levels while preserving the local and global information of graphs. Experimental results on multiple benchmark datasets demonstrate that the proposed MLC-GCN method is competitive with the state-of-the-art graph classification methods.},
	language = {en},
	urldate = {2022-06-22},
	journal = {Knowledge-Based Systems},
	author = {Xie, Yu and Yao, Chuanyu and Gong, Maoguo and Chen, Cheng and Qin, A. K.},
	month = apr,
	year = {2020},
	keywords = {Graph classification, Graph convolutional networks, Multi-level coarsening},
	pages = {105578},
}

@article{zhang_harp_2021,
	title = {{HARP} {Pro}: {Hierarchical} {Representation} {Learning} based on global and local features for social networks},
	shorttitle = {{HARP} {Pro}},
	author = {Zhang, Wei and Yang, Jing and Shang, Fanshu},
	year = {2021},
}

@inproceedings{bravo_hermsdorff_unifying_2019,
	title = {A {Unifying} {Framework} for {Spectrum}-{Preserving} {Graph} {Sparsification} and {Coarsening}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/cd474f6341aeffd65f93084d0dae3453-Abstract.html},
	urldate = {2022-06-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bravo Hermsdorff, Gecia and Gunderson, Lee},
	year = {2019},
}

@misc{li_deepergcn_2020,
	title = {{DeeperGCN}: {All} {You} {Need} to {Train} {Deeper} {GCNs}},
	shorttitle = {{DeeperGCN}},
	url = {http://arxiv.org/abs/2006.07739},
	doi = {10.48550/arXiv.2006.07739},
	abstract = {Graph Convolutional Networks (GCNs) have been drawing significant attention with the power of representation learning on graphs. Unlike Convolutional Neural Networks (CNNs), which are able to take advantage of stacking very deep layers, GCNs suffer from vanishing gradient, over-smoothing and over-fitting issues when going deeper. These challenges limit the representation power of GCNs on large-scale graphs. This paper proposes DeeperGCN that is capable of successfully and reliably training very deep GCNs. We define differentiable generalized aggregation functions to unify different message aggregation operations (e.g. mean, max). We also propose a novel normalization layer namely MsgNorm and a pre-activation version of residual connections for GCNs. Extensive experiments on Open Graph Benchmark (OGB) show DeeperGCN significantly boosts performance over the state-of-the-art on the large scale graph learning tasks of node property prediction and graph property prediction. Please visit https://www.deepgcns.org for more information.},
	urldate = {2022-06-22},
	publisher = {arXiv},
	author = {Li, Guohao and Xiong, Chenxin and Thabet, Ali and Ghanem, Bernard},
	month = jun,
	year = {2020},
	note = {Number: arXiv:2006.07739
arXiv:2006.07739 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{deng_graphzoom_2020,
	title = {{GraphZoom}: {A} {Multi}-level {Spectral} {Approach} for {Accurate} and {Scalable} {Graph} {Embedding}},
	shorttitle = {{GraphZoom}},
	url = {https://openreview.net/forum?id=r1lGO0EKDH},
	abstract = {A multi-level spectral approach to improving the quality and scalability of unsupervised graph embedding.},
	language = {en},
	urldate = {2022-06-23},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Deng, Chenhui and Zhao, Zhiqiang and Wang, Yongyu and Zhang, Zhiru and Feng, Zhuo},
	month = mar,
	year = {2020},
}

@inproceedings{cai_graph_2022,
	title = {Graph {Coarsening} with {Neural} {Networks}},
	url = {https://openreview.net/forum?id=uxpzitPEooJ},
	abstract = {As large scale-graphs become increasingly more prevalent, it poses significant computational challenges to process, extract and analyze large graph data. Graph coarsening is one popular technique...},
	language = {en},
	urldate = {2022-06-23},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Cai, Chen and Wang, Dingkang and Wang, Yusu},
	month = feb,
	year = {2022},
}

@article{fortin_deap_2012,
	title = {{DEAP}: {Evolutionary} {Algorithms} {Made} {Easy}},
	volume = {13},
	journal = {Journal of Machine Learning Research},
	author = {Fortin, F{\'e}lix-Antoine and Rainville, Fran{\c c}ois-Michel De and Gardner, Marc-Andr{\'e} and Parizeau, Marc and Gagn{\'e}, Christian},
	month = jul,
	year = {2012},
	pages = {2171--2175},
}

@inproceedings{prochazka_downstream_2022,
	address = {Zuberec, Slovakia},
	title = {Downstream {Task} {Aware} {Scalable} {Graph} {Size} {Reduction} for {Efficient} {GNN} {Application} on {Big} {Data}},
	booktitle = {Information {Technologies} - {Applications} and {Theory} ({ITAT} 2022)},
	author = {Proch{\'a}zka, Pavel and Mare{\v s}, Michal and D{\v e}di{\v c}, Marek},
	year = {2022},
}

@misc{kammer_space-efficient_2022,
	title = {Space-{Efficient} {Graph} {Coarsening} with {Applications} to {Succinct} {Planar} {Encodings}},
	abstract = {We present a novel space-efficient graph coarsening technique for \$n\$-vertex planar graphs \$G\$, called {\textbackslash}textit\{cloud partition\}, which partitions the vertices \$V(G)\$ into disjoint sets \$C\$ of size \$O({\textbackslash}log n)\$ such that each \$C\$ induces a connected subgraph of \$G\$. Using this partition \${\textbackslash}mathcal\{P\}\$ we construct a so-called {\textbackslash}textit\{structure-maintaining minor\} \$F\$ of \$G\$ via specific contractions within the disjoint sets such that \$F\$ has \$O(n/{\textbackslash}log n)\$ vertices. The combination of \$(F, {\textbackslash}mathcal\{P\})\$ is referred to as a {\textbackslash}textit\{cloud decomposition\}. For planar graphs we show that a cloud decomposition can be constructed in \$O(n)\$ time and using \$O(n)\$ bits. Given a cloud decomposition \$(F, {\textbackslash}mathcal\{P\})\$ constructed for a planar graph \$G\$ we are able to find a balanced separator of \$G\$ in \$O(n/{\textbackslash}log n)\$ time. Contrary to related publications, we do not make use of an embedding of the planar input graph. We generalize our cloud decomposition from planar graphs to \$H\$-minor-free graphs for any fixed graph \$H\$. This allows us to construct the succinct encoding scheme for \$H\$-minor-free graphs due to Blelloch and Farzan (CPM 2010) in \$O(n)\$ time and \$O(n)\$ bits improving both runtime and space by a factor of \${\textbackslash}Theta({\textbackslash}log n)\$. As an additional application of our cloud decomposition we show that, for \$H\$-minor-free graphs, a tree decomposition of width \$O(n{\textasciicircum}\{1/2 + {\textbackslash}epsilon\})\$ for any \${\textbackslash}epsilon {\textgreater} 0\$ can be constructed in \$O(n)\$ bits and a time linear in the size of the tree decomposition. A similar result by Izumi and Otachi (ICALP 2020) constructs a tree decomposition of width \$O(k {\textbackslash}sqrt\{n\} {\textbackslash}log n)\$ for graphs of treewidth \$k {\textbackslash}leq {\textbackslash}sqrt\{n\}\$ in sublinear space and polynomial time.},
	urldate = {2022-08-10},
	publisher = {arXiv},
	author = {Kammer, Frank and Meintrup, Johannes},
	month = jun,
	year = {2022},
	note = {arXiv:2205.06128},
	keywords = {Computer Science - Data Structures and Algorithms},
}

@misc{liu_comprehensive_2022,
	title = {Comprehensive {Graph} {Gradual} {Pruning} for {Sparse} {Training} in {Graph} {Neural} {Networks}},
	abstract = {Graph Neural Networks (GNNs) tend to suffer from high computation costs due to the exponentially increasing scale of graph data and the number of model parameters, which restricts their utility in practical applications. To this end, some recent works focus on sparsifying GNNs with the lottery ticket hypothesis (LTH) to reduce inference costs while maintaining performance levels. However, the LTH-based methods suffer from two major drawbacks: 1) they require exhaustive and iterative training of dense models, resulting in an extremely large training computation cost, and 2) they only trim graph structures and model parameters but ignore the node feature dimension, where significant redundancy exists. To overcome the above limitations, we propose a comprehensive graph gradual pruning framework termed CGP. This is achieved by designing a during-training graph pruning paradigm to dynamically prune GNNs within one training process. Unlike LTH-based methods, the proposed CGP approach requires no re-training, which significantly reduces the computation costs. Furthermore, we design a co-sparsifying strategy to comprehensively trim all three core elements of GNNs: graph structures, node features, and model parameters. Meanwhile, aiming at refining the pruning operation, we introduce a regrowth process into our CGP framework, in order to re-establish the pruned but important connections. The proposed CGP is evaluated by using a node classification task across 6 GNN architectures, including shallow models (GCN and GAT), shallow-but-deep-propagation models (SGC and APPNP), and deep models (GCNII and ResGCN), on a total of 14 real-world graph datasets, including large-scale graph datasets from the challenging Open Graph Benchmark. Experiments reveal that our proposed strategy greatly improves both training and inference efficiency while matching or even exceeding the accuracy of existing methods.},
	language = {en},
	urldate = {2022-08-12},
	publisher = {arXiv},
	author = {Liu, Chuang and Ma, Xueqi and Zhan, Yibing and Ding, Liang and Tao, Dapeng and Du, Bo and Hu, Wenbin and Mandic, Danilo},
	month = jul,
	year = {2022},
	note = {arXiv:2207.08629},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Liu et al. - 2022 - Comprehensive Graph Gradual Pruning for Sparse Tra.pdf:/home/cisco/Zotero/storage/N7N2RGPM/Liu et al. - 2022 - Comprehensive Graph Gradual Pruning for Sparse Tra.pdf:application/pdf},
}

@inproceedings{chiang_cluster-gcn_2019,
	address = {New York, NY, USA},
	series = {{KDD} '19},
	title = {Cluster-{GCN}: {An} {Efficient} {Algorithm} for {Training} {Deep} and {Large} {Graph} {Convolutional} {Networks}},
	isbn = {978-1-4503-6201-6},
	shorttitle = {Cluster-{GCN}},
	url = {https://doi.org/10.1145/3292500.3330925},
	doi = {10.1145/3292500.3330925},
	abstract = {Graph convolutional network (GCN) has been successfully applied to many graph-based applications; however, training a large-scale GCN remains challenging. Current SGD-based algorithms suffer from either a high computational cost that exponentially grows with number of GCN layers, or a large space requirement for keeping the entire graph and the embedding of each node in memory. In this paper, we propose Cluster-GCN, a novel GCN algorithm that is suitable for SGD-based training by exploiting the graph clustering structure. Cluster-GCN works as the following: at each step, it samples a block of nodes that associate with a dense subgraph identified by a graph clustering algorithm, and restricts the neighborhood search within this subgraph. This simple but effective strategy leads to significantly improved memory and computational efficiency while being able to achieve comparable test accuracy with previous algorithms. To test the scalability of our algorithm, we create a new Amazon2M data with 2 million nodes and 61 million edges which is more than 5 times larger than the previous largest publicly available dataset (Reddit). For training a 3-layer GCN on this data, Cluster-GCN is faster than the previous state-of-the-art VR-GCN (1523 seconds vs 1961 seconds) and using much less memory (2.2GB vs 11.2GB). Furthermore, for training 4 layer GCN on this data, our algorithm can finish in around 36 minutes while all the existing GCN training algorithms fail to train due to the out-of-memory issue. Furthermore, Cluster-GCN allows us to train much deeper GCN without much time and memory overhead, which leads to improved prediction accuracy---using a 5-layer Cluster-GCN, we achieve state-of-the-art test F1 score 99.36 on the PPI dataset, while the previous best result was 98.71 by{\textasciitilde}{\textbackslash}citezhang2018gaan.},
	urldate = {2022-11-28},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Chiang, Wei-Lin and Liu, Xuanqing and Si, Si and Li, Yang and Bengio, Samy and Hsieh, Cho-Jui},
	month = jul,
	year = {2019},
	keywords = {clustering, deep learning, graph convolutional networks, large-scale learning, semi-supervised learning},
	pages = {257--266},
}

@article{rozemberczki_multi-scale_2021,
	title = {Multi-{Scale} attributed node embedding},
	volume = {9},
	issn = {2051-1329},
	url = {https://doi.org/10.1093/comnet/cnab014},
	doi = {10.1093/comnet/cnab014},
	abstract = {We present network embedding algorithms that capture information about a node from the local distribution over node attributes around it, as observed over random walks following an approach similar to Skip-gram. Observations from neighbourhoods of different sizes are either pooled (AE) or encoded distinctly in a multi-scale approach (MUSAE). Capturing attribute-neighbourhood relationships over multiple scales is useful for a range of applications, including latent feature identification across disconnected networks with similar features. We prove theoretically that matrices of node-feature pointwise mutual information are implicitly factorized by the embeddings. Experiments show that our algorithms are computationally efficient and outperform comparable models on social networks and web graphs.},
	number = {2},
	urldate = {2022-11-28},
	journal = {Journal of Complex Networks},
	author = {Rozemberczki, Benedek and Allen, Carl and Sarkar, Rik},
	month = apr,
	year = {2021},
	pages = {cnab014},
}

@article{herrmann_multilevel_2019,
	title = {Multilevel {Algorithms} for {Acyclic} {Partitioning} of {Directed} {Acyclic} {Graphs}},
	volume = {41},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/abs/10.1137/18M1176865},
	doi = {10.1137/18M1176865},
	abstract = {We investigate the problem of partitioning the vertices of a directed acyclic graph into a given number of parts. The objective function is to minimize the number or the total weight of the edges having end points in different parts, which is also known as the edge cut. The standard load balancing constraint of having an equitable partition of the vertices among the parts should be met. Furthermore, the partition is required to be acyclic; i.e., the interpart edges between the vertices from different parts should preserve an acyclic dependency structure among the parts. In this work, we adopt the multilevel approach with coarsening, initial partitioning, and refinement phases for acyclic partitioning of directed acyclic graphs. We focus on two-way partitioning (sometimes called bisection), as this scheme can be used in a recursive way for multiway partitioning. To ensure the acyclicity of the partition at all times, we propose novel and efficient coarsening and refinement heuristics. The quality of the computed acyclic partitions is assessed by computing the edge cut. We also propose effective ways to use the standard undirected graph partitioning methods in our multilevel scheme. We perform a large set of experiments on a dataset consisting of (i) graphs coming from an application and (ii) some others corresponding to matrices from a public collection. We report significant improvements compared to the current state of the art.},
	number = {4},
	urldate = {2022-11-28},
	journal = {SIAM Journal on Scientific Computing},
	author = {Herrmann, Julien and {\"O}zkaya, M. Yusuf and U{\c c}ar, Bora and Kaya, Kamer and {\c C}ataly{\"u}rek, {\"U}mit V.},
	month = jan,
	year = {2019},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {05C70, 05C85, 68R10, 68W05, acyclic partitioning, directed graph, multilevel partitioning},
	pages = {A2117--A2145},
}

@article{bethune_hierarchical_2020,
	title = {Hierarchical and {Unsupervised} {Graph} {Representation} {Learning} with {Loukas}{\textquoteright}s {Coarsening}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1999-4893},
	url = {https://www.mdpi.com/1999-4893/13/9/206},
	doi = {10.3390/a13090206},
	abstract = {We propose a novel algorithm for unsupervised graph representation learning with attributed graphs. It combines three advantages addressing some current limitations of the literature: (i) The model is inductive: it can embed new graphs without re-training in the presence of new data; (ii) The method takes into account both micro-structures and macro-structures by looking at the attributed graphs at different scales; (iii) The model is end-to-end differentiable: it is a building block that can be plugged into deep learning pipelines and allows for back-propagation. We show that combining a coarsening method having strong theoretical guarantees with mutual information maximization suffices to produce high quality embeddings. We evaluate them on classification tasks with common benchmarks of the literature. We show that our algorithm is competitive with state of the art among unsupervised graph representation learning methods.},
	language = {en},
	number = {9},
	urldate = {2022-11-28},
	journal = {Algorithms},
	author = {B{\'e}thune, Louis and Kaloga, Yacouba and Borgnat, Pierre and Garivier, Aur{\'e}lien and Habrard, Amaury},
	month = sep,
	year = {2020},
	note = {Number: 9
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {graph coarsening, graph convolutional networks, graph representation learning, Graph2Vec, mutual information maximization, unsupervised learning},
	pages = {206},
}

@article{liu_hierarchical_2021,
	title = {Hierarchical {Adaptive} {Pooling} by {Capturing} {High}-order {Dependency} for {Graph} {Representation} {Learning}},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2021.3133646},
	abstract = {Graph neural networks (GNN) have been proven to be mature enough for handling graph-structured data on node-level graph representation learning tasks. However, the graph pooling technique for learning expressive graph-level representation is critical yet still challenging. Existing pooling methods either struggle to capture the local substructure or fail to effectively utilize high-order dependency, thus diminishing the expression capability. In this paper we propose HAP, a hierarchical graph-level representation learning framework, which is adaptively sensitive to graph structures, i.e., HAP clusters local substructures incorporating with high-order dependencies. HAP utilizes a novel cross-level attention mechanism MOA to naturally focus more on close neighborhood while effectively capture higher-order dependency that may contain crucial information. It also learns a global graph content GCont that extracts the graph pattern properties to make the pre- and post-coarsening graph content maintain stable, thus providing global guidance in graph coarsening. This novel innovation also facilitates generalization across graphs with the same form of features. Extensive experiments on ten datasets show that HAP significantly outperforms twelve popular graph pooling methods on graph classification task with an maximum accuracy improvement of 20.18\%, and exceeds the performance of state-of-the-art graph matching and graph similarity learning algorithms by over 3.42\% and 16\%.},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Liu, Ning and Jian, Songlei and Li, Dongsheng and Zhang, Yiming and Lai, Zhiquan and Xu, Hongzuo},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Attention Mechanism, Convolution, Feature extraction, Graph Pooling, Graph Representation Learning, Hierarchical Manner, Neural networks, Representation learning, Task analysis, Technological innovation, Topology},
	pages = {1--1},
}

@article{osei-kuffuor_matrix_2015,
	title = {Matrix  {Reordering} {Using} {Multilevel}  {Graph} {Coarsening} for {ILU} {Preconditioning}},
	volume = {37},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/abs/10.1137/130936610},
	doi = {10.1137/130936610},
	abstract = {Incomplete LU factorization (ILU) techniques are a well-known class of preconditioners, often used in conjunction with Krylov accelerators for the iterative solution of linear systems of equations. However, for certain problems, ILU factorizations can yield factors that are unstable and in some cases quite dense. Reordering techniques based on permuting the matrix prior to performing the factorization have been shown to improve the quality of the factorization, and the resulting preconditioner. In this paper, we examine the effect of reordering techniques based on multilevel graph coarsening ideas on one-level ILU factorizations, such as the level-based ILU(
k
k
) or the dual threshold ILUT algorithms. We consider an aggregation-based coarsening idea that implements two main coarsening frameworks---a top-down approach, and a bottom-up approach---each utilizing one of two different strategies to select the next-level coarse graph. Numerical results are presented to support our findings.},
	number = {1},
	urldate = {2022-11-28},
	journal = {SIAM Journal on Scientific Computing},
	author = {Osei-Kuffuor, Daniel and Li, Ruipeng and Saad, Yousef},
	month = jan,
	year = {2015},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {65F08, 65F10, 65F50, 65N22, 65Y20, algebraic preconditioners, ILU preconditioners, incomplete factorization preconditioners, multilevel graph coarsening, sparse matrix reordering},
	pages = {A391--A419},
}

@article{ubaru_sampling_2019,
	title = {Sampling and multilevel coarsening algorithms for fast matrix approximations},
	volume = {26},
	issn = {1099-1506},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nla.2234},
	doi = {10.1002/nla.2234},
	abstract = {This paper addresses matrix approximation problems for matrices that are large, sparse, and/or representations of large graphs. To tackle these problems, we consider algorithms that are based primarily on coarsening techniques, possibly combined with random sampling. A multilevel coarsening technique is proposed, which utilizes a hypergraph associated with the data matrix and a graph coarsening strategy based on column matching. We consider a number of standard applications of this technique as well as a few new ones. Among standard applications, we first consider the problem of computing partial singular value decomposition, for which a combination of sampling and coarsening yields significantly improved singular value decomposition results relative to sampling alone. We also consider the column subset selection problem, a popular low-rank approximation method used in data-related applications, and show how multilevel coarsening can be adapted for this problem. Similarly, we consider the problem of graph sparsification and show how coarsening techniques can be employed to solve it. We also establish theoretical results that characterize the approximation error obtained and the quality of the dimension reduction achieved by a coarsening step, when a proper column matching strategy is employed. Numerical experiments illustrate the performances of the methods in a few applications.},
	language = {en},
	number = {3},
	urldate = {2022-11-28},
	journal = {Numerical Linear Algebra with Applications},
	author = {Ubaru, Shashanka and Saad, Yousef},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/nla.2234},
	keywords = {coarsening, multilevel methods, randomization, singular values, subspace iteration, SVD},
	pages = {e2234},
}

@inproceedings{yuan_which_2021,
	address = {New York, NY, USA},
	series = {{GECCO} '21},
	title = {Which hyperparameters to optimise? an investigation of evolutionary hyperparameter optimisation in graph neural network for molecular property prediction},
	isbn = {978-1-4503-8351-6},
	shorttitle = {Which hyperparameters to optimise?},
	url = {https://doi.org/10.1145/3449726.3463192},
	doi = {10.1145/3449726.3463192},
	abstract = {Most GNNs for molecular property prediction are proposed based on the idea of learning the representations for the nodes by aggregating the information of their neighbour nodes in graph layers. Then, the representations can be passed to subsequent task-specific layers to deal with individual downstream tasks. Facing real-world molecular problems, the hyperparameter optimisation for those layers are vital. In this research, we focus on the impact of selecting two types of GNN hyperparameters, those belonging to graph layers and those of task-specific layers, on the performance of GNN for molecular property prediction. In our experiments, we employed a state-of-the-art evolutionary algorithm (i.e., CMA-ES) for HPO. The results reveal that optimising the two types of hyperparameters separately can improve GNNs' performance, but optimising both types of hyperparameters simultaneously will lead to predominant improvements.},
	urldate = {2022-11-28},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference} {Companion}},
	publisher = {Association for Computing Machinery},
	author = {Yuan, Yingfang and Wang, Wenjun and Pang, Wei},
	month = jul,
	year = {2021},
	keywords = {evolutionary computation, graph neural networks, hyperparameter optimisation, molecular property prediction},
	pages = {1403--1404},
}

@inproceedings{catalyurek_multithreaded_2012,
	title = {Multithreaded {Clustering} for {Multi}-level {Hypergraph} {Partitioning}},
	doi = {10.1109/IPDPS.2012.81},
	abstract = {Requirements for efficient parallelization of many complex and irregular applications can be cast as a hyper graph partitioning problem. The current-state-of-the art software libraries that provide tool support for the hyper graph partitioning problem are designed and implemented before the game-changing advancements in multi-core computing. Hence, analyzing the structure of those tools for designing multithreaded versions of the algorithms is a crucial tasks. The most successful partitioning tools are based on the multi-level approach. In this approach, a given hyper graph is coarsened to a much smaller one, a partition is obtained on the the smallest hyper graph, and that partition is projected to the original hyper graph while refining it on the intermediate hyper graphs. The coarsening operation corresponds to clustering the vertices of a hyper graph and is the most time consuming task in a multi-level partitioning tool. We present three efficient multithreaded clustering algorithms which are very suited for multi-level partitioners. We compare their performance with that of the ones currently used in today's hyper graph partitioners. We show on a large number of real life hyper graphs that our implementations, integrated into a commonly used partitioning library PaToH, achieve good speedups without reducing the clustering quality.},
	booktitle = {2012 {IEEE} 26th {International} {Parallel} and {Distributed} {Processing} {Symposium}},
	author = {{\c C}ataly{\"u}rek, {\"U}mit V. and Deveci, Mehmet and Kaya, Kamer and U{\c c}ar, Bora},
	month = may,
	year = {2012},
	note = {ISSN: 1530-2075},
	keywords = {Algorithm design and analysis, Approximation algorithms, Clustering algorithms, coarsening, Heuristic algorithms, Measurement, Multi-level hypergraph partitioning, multicore programming, multithreaded clustering algorithms, Partitioning algorithms, Pins},
	pages = {848--859},
}
