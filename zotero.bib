
@article{kingma_adam:_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2020-06-25},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Learning, Computer Science - Machine Learning},
}

@article{chen_harp_2018,
	title = {{HARP}: {Hierarchical} {Representation} {Learning} for {Networks}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	shorttitle = {{HARP}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11849},
	language = {en},
	number = {1},
	urldate = {2021-05-25},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chen, Haochen and Perozzi, Bryan and Hu, Yifan and Skiena, Steven},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {network classification},
}

@inproceedings{schulz_mining_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Mining {Tree} {Patterns} with {Partially} {Injective} {Homomorphisms}},
	isbn = {978-3-030-10928-8},
	doi = {10.1007/978-3-030-10928-8_35},
	abstract = {One of the main differences between inductive logic programming (ILP) and graph mining lies in the pattern matching operator applied: While it is mainly defined by relational homomorphism (i.e., subsumption) in ILP, subgraph isomorphism is the most common pattern matching operator in graph mining. Using the fact that subgraph isomorphisms are injective homomorphisms, we bridge the gap between ILP and graph mining by considering a natural transition from homomorphisms to subgraph isomorphisms that is defined by partially injective homomorphisms, i.e., which require injectivity only for subsets of the vertex pairs in the pattern. Utilizing positive complexity results on deciding homomorphisms from bounded tree-width graphs, we present an algorithm mining frequent trees from arbitrary graphs w.r.t. partially injective homomorphisms. Our experimental results show that the predictive performance of the patterns obtained is comparable to that of ordinary frequent subgraphs. Thus, by preserving much from the advantageous properties of homomorphisms and subgraph isomorphisms, our approach provides a trade-off between efficiency and predictive power.},
	language = {en},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer International Publishing},
	author = {Schulz, Till Hendrik and Horváth, Tamás and Welke, Pascal and Wrobel, Stefan},
	editor = {Berlingerio, Michele and Bonchi, Francesco and Gärtner, Thomas and Hurley, Neil and Ifrim, Georgiana},
	year = {2019},
	pages = {585--601},
}

@inproceedings{tang_line_2015,
	title = {Line: {Large}-scale information network embedding},
	shorttitle = {Line},
	booktitle = {Proceedings of the 24th international conference on world wide web},
	author = {Tang, Jian and Qu, Meng and Wang, Mingzhe and Zhang, Ming and Yan, Jun and Mei, Qiaozhu},
	year = {2015},
	pages = {1067--1077},
}

@inproceedings{perozzi_deepwalk_2014,
	title = {Deepwalk: {Online} learning of social representations},
	shorttitle = {Deepwalk},
	booktitle = {Proceedings of the 20th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	author = {Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
	year = {2014},
	pages = {701--710},
}

@inproceedings{grover_node2vec_2016,
	title = {node2vec: {Scalable} feature learning for networks},
	shorttitle = {node2vec},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	author = {Grover, Aditya and Leskovec, Jure},
	year = {2016},
	pages = {855--864},
}

@article{hu_open_2021,
	title = {Open {Graph} {Benchmark}: {Datasets} for {Machine} {Learning} on {Graphs}},
	shorttitle = {Open {Graph} {Benchmark}},
	url = {http://arxiv.org/abs/2005.00687},
	abstract = {We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale, encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at https://ogb.stanford.edu .},
	urldate = {2021-06-03},
	journal = {arXiv:2005.00687 [cs, stat]},
	author = {Hu, Weihua and Fey, Matthias and Zitnik, Marinka and Dong, Yuxiao and Ren, Hongyu and Liu, Bowen and Catasta, Michele and Leskovec, Jure},
	month = feb,
	year = {2021},
	note = {arXiv: 2005.00687},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
}

@article{zhang_eigen-gnn_2020,
	title = {Eigen-{GNN}: {A} {Graph} {Structure} {Preserving} {Plug}-in for {GNNs}},
	shorttitle = {Eigen-{GNN}},
	url = {http://arxiv.org/abs/2006.04330},
	abstract = {Graph Neural Networks (GNNs) are emerging machine learning models on graphs. Although sufficiently deep GNNs are shown theoretically capable of fully preserving graph structures, most existing GNN models in practice are shallow and essentially feature-centric. We show empirically and analytically that the existing shallow GNNs cannot preserve graph structures well. To overcome this fundamental challenge, we propose Eigen-GNN, a simple yet effective and general plug-in module to boost GNNs ability in preserving graph structures. Specifically, we integrate the eigenspace of graph structures with GNNs by treating GNNs as a type of dimensionality reduction and expanding the initial dimensionality reduction bases. Without needing to increase depths, Eigen-GNN possesses more flexibilities in handling both feature-driven and structure-driven tasks since the initial bases contain both node features and graph structures. We present extensive experimental results to demonstrate the effectiveness of Eigen-GNN for tasks including node classification, link prediction, and graph isomorphism tests.},
	urldate = {2021-06-04},
	journal = {arXiv:2006.04330 [cs, stat]},
	author = {Zhang, Ziwei and Cui, Peng and Pei, Jian and Wang, Xin and Zhu, Wenwu},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.04330},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{huang_combining_2020,
	title = {Combining {Label} {Propagation} and {Simple} {Models} {Out}-performs {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2010.13993},
	abstract = {Graph Neural Networks (GNNs) are the predominant technique for learning over graphs. However, there is relatively little understanding of why GNNs are successful in practice and whether they are necessary for good performance. Here, we show that for many standard transductive node classification benchmarks, we can exceed or match the performance of state-of-the-art GNNs by combining shallow models that ignore the graph structure with two simple post-processing steps that exploit correlation in the label structure: (i) an "error correlation" that spreads residual errors in training data to correct errors in test data and (ii) a "prediction correlation" that smooths the predictions on the test data. We call this overall procedure Correct and Smooth (C\&S), and the post-processing steps are implemented via simple modifications to standard label propagation techniques from early graph-based semi-supervised learning methods. Our approach exceeds or nearly matches the performance of state-of-the-art GNNs on a wide variety of benchmarks, with just a small fraction of the parameters and orders of magnitude faster runtime. For instance, we exceed the best known GNN performance on the OGB-Products dataset with 137 times fewer parameters and greater than 100 times less training time. The performance of our methods highlights how directly incorporating label information into the learning algorithm (as was done in traditional techniques) yields easy and substantial performance gains. We can also incorporate our techniques into big GNN models, providing modest gains. Our code for the OGB results is at https://github.com/Chillee/CorrectAndSmooth.},
	urldate = {2021-06-04},
	journal = {arXiv:2010.13993 [cs]},
	author = {Huang, Qian and He, Horace and Singh, Abhay and Lim, Ser-Nam and Benson, Austin R.},
	month = nov,
	year = {2020},
	note = {arXiv: 2010.13993},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks},
}

@article{salha_keep_2019,
	title = {Keep {It} {Simple}: {Graph} {Autoencoders} {Without} {Graph} {Convolutional} {Networks}},
	shorttitle = {Keep {It} {Simple}},
	url = {http://arxiv.org/abs/1910.00942},
	abstract = {Graph autoencoders (AE) and variational autoencoders (VAE) recently emerged as powerful node embedding methods, with promising performances on challenging tasks such as link prediction and node clustering. Graph AE, VAE and most of their extensions rely on graph convolutional networks (GCN) to learn vector space representations of nodes. In this paper, we propose to replace the GCN encoder by a simple linear model w.r.t. the adjacency matrix of the graph. For the two aforementioned tasks, we empirically show that this approach consistently reaches competitive performances w.r.t. GCN-based models for numerous real-world graphs, including the widely used Cora, Citeseer and Pubmed citation networks that became the de facto benchmark datasets for evaluating graph AE and VAE. This result questions the relevance of repeatedly using these three datasets to compare complex graph AE and VAE models. It also emphasizes the effectiveness of simple node encoding schemes for many real-world applications.},
	urldate = {2021-06-04},
	journal = {arXiv:1910.00942 [cs, stat]},
	author = {Salha, Guillaume and Hennequin, Romain and Vazirgiannis, Michalis},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.00942},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
}

@article{frasca_sign_2020,
	title = {{SIGN}: {Scalable} {Inception} {Graph} {Neural} {Networks}},
	shorttitle = {{SIGN}},
	url = {http://arxiv.org/abs/2004.11198},
	abstract = {Graph representation learning has recently been applied to a broad spectrum of problems ranging from computer graphics and chemistry to high energy physics and social media. The popularity of graph neural networks has sparked interest, both in academia and in industry, in developing methods that scale to very large graphs such as Facebook or Twitter social networks. In most of these approaches, the computational cost is alleviated by a sampling strategy retaining a subset of node neighbors or subgraphs at training time. In this paper we propose a new, efficient and scalable graph deep learning architecture which sidesteps the need for graph sampling by using graph convolutional filters of different size that are amenable to efficient precomputation, allowing extremely fast training and inference. Our architecture allows using different local graph operators (e.g. motif-induced adjacency matrices or Personalized Page Rank diffusion matrix) to best suit the task at hand. We conduct extensive experimental evaluation on various open benchmarks and show that our approach is competitive with other state-of-the-art architectures, while requiring a fraction of the training and inference time. Moreover, we obtain state-of-the-art results on ogbn-papers100M, the largest public graph dataset, with over 110 million nodes and 1.5 billion edges.},
	urldate = {2021-06-04},
	journal = {arXiv:2004.11198 [cs, stat]},
	author = {Frasca, Fabrizio and Rossi, Emanuele and Eynard, Davide and Chamberlain, Ben and Bronstein, Michael and Monti, Federico},
	month = nov,
	year = {2020},
	note = {arXiv: 2004.11198},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {8024--8035},
}

@article{fey_fast_2019,
	title = {Fast {Graph} {Representation} {Learning} with {PyTorch} {Geometric}},
	url = {http://arxiv.org/abs/1903.02428},
	abstract = {We introduce PyTorch Geometric, a library for deep learning on irregularly structured input data such as graphs, point clouds and manifolds, built upon PyTorch. In addition to general graph data structures and processing methods, it contains a variety of recently published methods from the domains of relational learning and 3D data processing. PyTorch Geometric achieves high data throughput by leveraging sparse GPU acceleration, by providing dedicated CUDA kernels and by introducing efficient mini-batch handling for input examples of different size. In this work, we present the library in detail and perform a comprehensive comparative study of the implemented methods in homogeneous evaluation scenarios.},
	urldate = {2021-06-22},
	journal = {arXiv:1903.02428 [cs, stat]},
	author = {Fey, Matthias and Lenssen, Jan Eric},
	month = apr,
	year = {2019},
	note = {arXiv: 1903.02428},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
}

@article{zhou_generalized_2020,
	title = {A generalized hierarchical co-{Kriging} model for multi-fidelity data fusion},
	volume = {62},
	issn = {1615-1488},
	url = {https://doi.org/10.1007/s00158-020-02583-7},
	doi = {10.1007/s00158-020-02583-7},
	abstract = {Multi-fidelity (MF) surrogate models have shown great potential in simulation-based design since they can make a trade-off between high prediction accuracy and low computational cost by augmenting the small number of expensive high-fidelity (HF) samples with a large number of cheap low-fidelity (LF) data. In this work, a generalized hierarchical co-Kriging (GCK) surrogate model is proposed for MF data fusion with both nested and non-nested sampling data. Specifically, a comprehensive Gaussian process (GP) Bayesian framework is developed by aggregating calibrated LF Kriging model and discrepancy stochastic Kriging model. The stochastic Kriging model enables the GCK model to consider the predictive uncertainty from the LF Kriging model at HF sampling points, making it possible to estimate the model parameter separately under both nested and non-nested sampling data. The performance of the GCK model is compared with three well-known Kriging-based MF surrogates, i.e., hybrid Kriging–scaling (HKS) model, KOH autoregressive (KOH) model, and hierarchical Kriging (HK) model, by testing them on two numerical examples and two real-life cases. The influence of correlations between LF and HF samples and the cost ratio between them are also analyzed. Comparison results on the illustrated cases demonstrate that the proposed GCK model shows great potential in MF modeling under non-nested sampling data, especially when the correlations between LF and HF samples are weak.},
	language = {en},
	number = {4},
	urldate = {2021-06-29},
	journal = {Structural and Multidisciplinary Optimization},
	author = {Zhou, Qi and Wu, Yuda and Guo, Zhendong and Hu, Jiexiang and Jin, Peng},
	month = oct,
	year = {2020},
	pages = {1885--1904},
}

@article{yang_conditional_2019,
	title = {Conditional deep surrogate models for stochastic, high-dimensional, and multi-fidelity systems},
	volume = {64},
	issn = {1432-0924},
	url = {https://doi.org/10.1007/s00466-019-01718-y},
	doi = {10.1007/s00466-019-01718-y},
	abstract = {We present a probabilistic deep learning methodology that enables the construction of predictive data-driven surrogates for stochastic systems. Leveraging recent advances in variational inference with implicit distributions, we put forth a statistical inference framework that enables the end-to-end training of surrogate models on paired input–output observations that may be stochastic in nature, originate from different information sources of variable fidelity, or be corrupted by complex noise processes. The resulting surrogates can accommodate high-dimensional inputs and outputs and are able to return predictions with quantified uncertainty. The effectiveness our approach is demonstrated through a series of canonical studies, including the regression of noisy data, multi-fidelity modeling of stochastic processes, and uncertainty propagation in high-dimensional dynamical systems.},
	language = {en},
	number = {2},
	urldate = {2021-06-29},
	journal = {Computational Mechanics},
	author = {Yang, Yibo and Perdikaris, Paris},
	month = aug,
	year = {2019},
	pages = {417--434},
}

@article{park_low-fidelity_2018,
	title = {Low-fidelity scale factor improves {Bayesian} multi-fidelity prediction by reducing bumpiness of discrepancy function},
	volume = {58},
	issn = {1615-1488},
	url = {https://doi.org/10.1007/s00158-018-2031-2},
	doi = {10.1007/s00158-018-2031-2},
	abstract = {This study explores why the use of the low-fidelity scale factor can substantially improve the accuracy of the Bayesian multi-fidelity surrogate (MFS). It is shown analytically that the Bayesian MFS framework utilizes the scale factor to reduce the waviness and variation of the discrepancy function by maximizing the Gaussian process-based likelihood function. Less wavy functions are more accurately fitted, and variation reduction mitigates the effect of fitting error. Bumpiness is another way used to combine waviness and variation. Two examples, Borehole3 and Hartmann6, illustrated that indeed the Bayesian MFS reduced bumpiness using the scale factor. The finding may be useful for MFS using surrogates lacking uncertainty structure, so that likelihood is not an option, but bumpiness may be.},
	language = {en},
	number = {2},
	urldate = {2021-06-29},
	journal = {Structural and Multidisciplinary Optimization},
	author = {Park, Chanyoung and Haftka, Raphael T. and Kim, Nam H.},
	month = aug,
	year = {2018},
	pages = {399--414},
}

@incollection{tang_pte_2015,
	address = {New York, NY, USA},
	title = {{PTE}: {Predictive} {Text} {Embedding} through {Large}-scale {Heterogeneous} {Text} {Networks}},
	isbn = {978-1-4503-3664-2},
	shorttitle = {{PTE}},
	url = {https://doi.org/10.1145/2783258.2783307},
	abstract = {Unsupervised text embedding methods, such as Skip-gram and Paragraph Vector, have been attracting increasing attention due to their simplicity, scalability, and effectiveness. However, comparing to sophisticated deep learning architectures such as convolutional neural networks, these methods usually yield inferior results when applied to particular machine learning tasks. One possible reason is that these text embedding methods learn the representation of text in a fully unsupervised way, without leveraging the labeled information available for the task. Although the low dimensional representations learned are applicable to many different tasks, they are not particularly tuned for any task. In this paper, we fill this gap by proposing a semi-supervised representation learning method for text data, which we call the predictive text embedding (PTE). Predictive text embedding utilizes both labeled and unlabeled data to learn the embedding of text. The labeled information and different levels of word co-occurrence information are first represented as a large-scale heterogeneous text network, which is then embedded into a low dimensional space through a principled and efficient algorithm. This low dimensional embedding not only preserves the semantic closeness of words and documents, but also has a strong predictive power for the particular task. Compared to recent supervised approaches based on convolutional neural networks, predictive text embedding is comparable or more effective, much more efficient, and has fewer parameters to tune.},
	urldate = {2021-06-30},
	booktitle = {Proceedings of the 21th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Tang, Jian and Qu, Meng and Mei, Qiaozhu},
	month = aug,
	year = {2015},
	keywords = {predictive text embedding, representation learning},
	pages = {1165--1174},
}

@inproceedings{topping_understanding_2021,
	title = {Understanding over-squashing and bottlenecks on graphs via curvature},
	url = {https://openreview.net/forum?id=7UmjRGzp-A},
	abstract = {Most graph neural networks (GNNs) use the message passing paradigm, in which node features are propagated on the input graph. Recent works pointed to the distortion of information flowing from...},
	language = {en},
	urldate = {2022-06-12},
	author = {Topping, Jake and Giovanni, Francesco Di and Chamberlain, Benjamin Paul and Dong, Xiaowen and Bronstein, Michael M.},
	month = sep,
	year = {2021},
}

@misc{velickovic_geometric_2021,
	title = {Geometric {Deep} {Learning} - {Grids}, {Groups}, {Graphs}, {Geodesics}, and {Gauges}},
	url = {https://geometricdeeplearning.com/geometricdeeplearning.com/},
	abstract = {Grids, Groups, Graphs, Geodesics, and Gauges},
	urldate = {2022-06-12},
	author = {Veličković, Petar},
	year = {2021},
}
