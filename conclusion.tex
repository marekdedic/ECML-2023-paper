\section{Conclusion}

In this work, the HARP algorithm was presented and generalized into an all-purpose graph coarsening schema. A novel way of prolonging graphs in the HARP setting was presented, yielding an adaptive algorithm that selectively prolongs the graph in a way that maximizes performance under limited graph size. Additionaly, 3 alternatives to HARP coarsening were presented, two based on graph diffusion and one based on evolutionary algorithms. Together, these two improvements substantially increase the versatility of HARP, turning it from a method for pre-training into a framework for graph reduction. Such a framework enables the study of properties of particular graphs, making it possible to reveal global structures. The framework may be used for lowering computational demands while preserving downstream task performance as well.

All of the proposed methods were experimentally verified. While the behaviour differs between the graphs studied, in general, our experiments reveal that at about X\%\todo{Fill in} reduction in node count, the accuracy of transductive node classification was not significantly different from accuracy on the full graph. In fact, at 50\% reduction in node count, the accuracy was still reasonably close to the accuracy on a full graph on most datasets. Additionally, the coarsenings based on graph diffusion were shown to outperform the original coarsening, again with the exact difference depending on the particular dataset.

In future work, some parts of the proposed method may be substantially simplified. This applies mostly to the adaptive coarsening schema, where a surrogate metric could be used to guide the prolongation, instead of evaluating the downstream task, which may be prohibitively computationally expensive. Our work-in-progress \todo{Citovat Pavl≈Øv paper} explores this direction in the setting of direct adaptive coarsening, instead of a fixed coarsening followed by adaptive prolongation. A similar simplification may be applied to the evolved coarsening, where a surrogate fitness function may also substantially lower computational costs. Regarding the proposed alternative coarsening schemas, hyper-parameter optimization techniques such as grid-search could be used to fine-tune the methods. For the evolved coarsening, encoding the coarsening parameters into the population will make the algorithm less sensitive to the initial settings with the added cost of increasing the dimension of the optimization task. Of interest is also comparison of different choices of genetic operators as well as a multi-criterial optimization with an additional criterion defined, e.g., as the reduction ratio of the input graph size.
