%!TEX TS-program = lualatex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[sn-mathphys,pdflatex]{sn-jnl}% Math and Physical Sciences Reference Style

%%%% Standard Packages
\usepackage{polyglossia} % Must come before biblatex
\usepackage{hyperref}

\setdefaultlanguage{english}
\setotherlanguage{czech}

\begin{document}

\section*{Information sheet}

Submission: Balancing performance and complexity with adaptive graph coarsening

\noindent Authors: Marek Dědič, Lukáš Bajer, Pavel Procházka and Martin Holeňa

\subsection*{Main claim}

The paper presents two extensions of HARP, an algorithm for pretraining GNNs on coarser graphs. Our extensions modify HARP to serve a different purpose -- enabling its use for reduction of graph size while maintaining most of the performance of the GNN. This reduction in graph size in turn reduces the computational and memory requirements of GNN training and enables the use of GNNs for graphs that were previously too large to use with GNNs.

\subsection*{Evidence}
The proposed approaches were experimentally evaluated and compared on 10 publicly available datasets. The main proposal of the paper, that is the ability to significantly reduce graph size without a major reduction of GNN performance was confirmed and statistically validated, with a 50\% reduction in graph size incurring only a 5\% reduction in model performance.

\subsection*{Related contributions}

\begin{itemize}
  \item \cite{chen_harp_2018} was the starting point for our reported research. Compared to the original  method presented in \cite{chen_harp_2018}, we added novel extensions in 3 directions -- adaptive approach to the prolongation phase, coarsening based on graph diffusion, using two different graph diffusion methods, and investigation of relationships of graph coarsening to graph homophily.
  \item \cite{huang_scaling_2021} uses graph coarsening from the point of view of the tradeoff between classification accuracy and computational complexity, which was the motivation for our research. However, it does not rely on \cite{chen_harp_2018} and does not include our adaptive approach nor the investigation of relationships to graph homophily.
  \item \cite{bethune_hierarchical_2020} is related to our research through considering graph coarsening on different scales -- macrostructures and microstructures. Similarly to \cite{huang_scaling_2021}, it does not rely on \cite{chen_harp_2018} and does not include our adaptive approach nor the investigation of relationships to graph homophily.
  \item \cite{xie_graph_2020} uses, similarly to our research, an adaptive multiscale coarsening approach. However, it does not rely on \cite{chen_harp_2018}, does not include the investigation of relationships to graph homophily, and the considered downstream application is not node coarsening, but graph coarsening.
  \item \cite{zhang_harp_2021} is also an extension of \cite{chen_harp_2018}, though focusing on community detection. It does not include our adaptive approach nor the investigation of relationships to graph homophily.
\end{itemize}

\subsection*{Keywords}

Graph representation learning, Graph coarsening, Graph diffusion, Graph homophily, Performance-complexity trade-off, HARP

\subsection*{Conflicts of interest}

E-mail domains of all institutions with which the authors have an institutional conflict of interest: \href{cisco.com}{cisco.com}, \href{cs.cas.cz}{cs.cas.cz}, \href{cvut.cz}{cvut.cz}, \href{uni-rostock.de}{uni-rostock.de}.

\subsection*{Previous publication}

The paper hasn't been published in any conference or journal. A preliminary version of this research work was presented at the Graph-Quality workshop at ECML PKDD 2022, however, the workshop didn't publish any proceedings.

\subsection*{Appropriate reviewers}

\begin{itemize}
  \item \textbf{Haochen Chen}, Stony Brook University, New York, USA, haocchen@cs.stonybrook.edu. He is the main author of the HARP approach, on which we heavily rely and which we further develop.
  \item \textbf{Johannes Gasteiger}, Technical University of Munich, Germany, j.gasteiger@in.tum.de. He is the main author of graph diffusion convolution, on which our most successful method for graph coarsening is based.
  \item \textbf{Kamer Kaya}, Sabanci University, Istanbul, Turkey, kaya@sabanciuniv.edu. He is an expert in graph coarsening, coauthor of several important papers establishing connections of graph coarsening with graph embedding and graph partitioning.
  \item \textbf{Yousef Saad}, University of Minnesota, Minneapolis, USA, saad@umn.edu. He is an expert in graph coarsening, coauthor of several important papers about multilevel graph coarsening in linear algebra, but also about the applicability of graph coarsening methods developed for linear algebra to machine learning, including graph neural networks. 
\end{itemize}

\subsection*{Potential reviewers with competing interests}

None.

\bibliography{zotero}

\end{document}
