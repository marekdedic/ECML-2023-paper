\section{Our method}

\subsection{Balancing performance and complexity}

\todo{This is old and needs to be rewritten}
Graph-based methods such as node2vec typically have a large number of parameters - on the widely used OGBN-ArXiv\todo{Don't mention OGB if no evaluation on OGB} dataset (see \cite{hu_open_2021}), the state-of-the-art node2vec model has over 21 million parameters. At the same time, recent works in the domain of graph learning have started to focus more heavily on simpler methods as a competitive alternative to heavy-weight ones (see \cite{frasca_sign_2020,huang_combining_2020,salha_keep_2019,zhang_eigen-gnn_2020}). As the authors of \cite{chen_harp_2018} observed, HARP improves the performance of models when fewer labelled data are available. The proposed lower complexity models based on HARP could also improve performance in a setting where only low fidelity data are available for large parts of the graph. Coarser models could be trained on them, with a subsequent training of finer models using only a limited sample of high fidelity data.

In this work, we examine two ways of leveraging the graph structure to study the performance-complexity characteristics of the graph data. Firstly, we replace the simple prolongation approach by an adaptive prolongation algorithm. Secondly, we study alternative ways of coarsening the graph.

\subsection{The adaptive prolongation approach}
Explanation of the adaptive prolongation approach

\subsection{More general approaches to coarsening}

\subsubsection{The PIHom framework}
Explain the general framework for coarsenings\todo{Only do this, if we can relate to it in any other parts of the paper}

\subsubsection{Graph diffusion coarsening}

\subsubsection{Evolved coarsening}
