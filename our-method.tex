\section{Our method}

\subsection{Balancing performance and complexity}

\todo{This is old and needs to be rewritten}
Graph-based methods such as node2vec typically have a large number of parameters - on the widely used OGBN-ArXiv\todo{Don't mention OGB if no evaluation on OGB} dataset (see \cite{hu_open_2021}), the state-of-the-art node2vec model has over 21 million parameters. At the same time, recent works in the domain of graph learning have started to focus more heavily on simpler methods as a competitive alternative to heavy-weight ones (see \cite{frasca_sign_2020,huang_combining_2020,salha_keep_2019,zhang_eigen-gnn_2020}). As the authors of \cite{chen_harp_2018} observed, HARP improves the performance of models when fewer labelled data are available. The proposed lower complexity models based on HARP could also improve performance in a setting where only low fidelity data are available for large parts of the graph. Coarser models could be trained on them, with a subsequent training of finer models using only a limited sample of high fidelity data.

In this work, we examine two ways of leveraging the graph structure to study the performance-complexity characteristics of the graph data. Firstly, we replace the simple prolongation approach by an adaptive prolongation algorithm. Secondly, we study alternative ways of coarsening the graph.

\subsection{The adaptive prolongation approach}

In standard HARP, once the coarsened graphs are obtained, the way to train the graph embedding is fairly straightforward. Starting with the coarsest graph, an embedding model such as node2vec is trained for a set amount of training time (for example, epochs). Following that, a step to a graph that is one level finer is made. The embedding learned on the previous graph is \enquote{prolonged}, that is the representations of merged nodes are reused. Then, with this prolonged embedding as the starting state, the embedding algorithm continues training and this process is repeated until reaching the original graph.

While this style of prolongation is fine when HARP is used only as a means of pre-training, this approach is far too crude when studying the underlying graph and its performance-complexity characteristics. For example, the widely-used Cora dataset (see \cite{yang_revisiting_2016}) has in its original form 2708 nodes, while the graph resulting from one application of the HARP coarsening schema has only about 1100 nodes (exact numbers may differ run-by-run). Such a relatively high reduction ratio effectively prevents any closer study of the graph quality for the different steps.

In order to offer a more fine-grained observation of the graph complexity and its effect on the downstream task, we present the adaptive prolongation approach. This algorithm works with the pre-coarsened graphs produced by e.g. HARP, however, the embedding is learned in a different manner.

\todo[inline]{Tohle sem moc nesedí, ale přijde mi to důležitý pro zasazení do kontextu - možná zahrnout do 4.1?}\todo[inline]{Tohle ale souvisí s PIHom - to je jen jinak formulované prohledávání toho svazu z PIHom!}Generally speaking, it is reasonable to assume that a graph coarsening only merges nodes belonging to the same connected component of the graph (and the coarsening scheme presented in \cite{chen_harp_2018} does, in fact, follow this rule). Under such an assumption, exhaustively applying the coarsening scheme until there are no more nodes the merge would yield a graph with one node per connected component of the original graph. Such a graph can be viewed as one boundary on the space of all possible coarsenings of a graph. The other boundary\todo{The boundaries are the top and the bottom of the lattice} is the original graph itself, with all coarsened versions of the graph inhabiting the space somewhere between these two boundaries. In ideal conditions, to study the effect of graph coarsenings on its quality (viewed through the lens of a downstream task), one would exhaustively study this space. In practice, such an exhaustive search is computationally infeasible, in fact, it is NP-complete\todo{This is motivated by PIHom}.

\todo[inline]{Tady jsem se vydal někam hluboko do teorie, vlastně nevím, jestli to má smysl, ale přišlo mi to zajímavý. Tak to možná ještě celý smažu.}The coarsenings of a graph with an algorithm such as HARP can, however, serve as a kind of \enquote{map} for the set of all coarsenings. Because the set is complete lattice and the coarsened graphs are constructed from one another consecutively, the set of all intermediary graphs together with the two boundaries form a chain in this complete lattice. Since the adaptive prolongation approach in each step prolongs the furthest node merge concerning a particular node, each step in the adaptive prolongation will be a point which lies on a chain containing all of the original coarsened graphs. In effect, this means that the adaptive prolongation approach searches not the set of all graph coarsenings, only the set of all coarsenings which lie on any chain containing all the graphs from the original coarsening algorithm. This in turn means that the quality of the adaptive prolongation is dependent on the quality of the underlying coarsening, a concept further explored in Subsection \ref{sec:coarsening-algorithms}

\todo{Podle mě je to dost nesrozumitelný, přidal bych pseudokód?}The adaptive prolongation approach uses the pre-computed coarsened graphs as a way to progressively increase the number of nodes with which the embedding is trained. However, the iterations of embedding training and prolongation are decoupled from the pre-computed coarsened graphs. Instead, in each step of the training, the current embedding is used to train a node classifier which guides which nodes should be prolonged. The node classifier is equivalent to the one which is to eventually be used for the downstream task and is trained on the same training subset of graph nodes. Using this classifier, a measure needs to be produced that guides the prolongation - ideally, only the nodes where the classifier performs the worst would be prolonged. Since such a measure needs to be computed for all nodes in the dataset, even the ones in the test set, the performance of the classifier cannot be taken directly. Instead, the confidence of this classifier is used, as it can be computed for each node. In our experiments, several methods of assessing the confidence were tried, ultimately settling on the entropy of the output of the softmax layer for each node - representing the amount of information the classifier is able to infer about each node. Nodes with the highest entropy were taken and a node merge in which they were involved in the pre-computed coarsening was found, prefering merges that happen on the coarsest graph. A set amount of such node merges is undone in each prolongation step, gradually prolonging until reaching the original graph.

\subsection{More general approaches to coarsening}\label{sec:coarsening-algorithms}

\subsubsection{The PIHom framework}
Explain the general framework for coarsenings\todo{Only do this, if we can relate to it in any other parts of the paper}

\subsubsection{Graph diffusion coarsening}

\subsubsection{Evolved coarsening}
