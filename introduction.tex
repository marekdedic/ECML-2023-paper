\section{Introduction}
Across a wide variety of applications and domains, graphs emerge as a domain-independent and ubiquitous way of organizing data. Consequently, machine learning on graphs has, in recent years, seen an explosion in popularity, breadth and depth of both research and applications. While there have been significant advances in algorithms for learning from graph data \cite{defferrard_convolutional_2016, kipf_semi-supervised_2017}, the structure of the underlying data has, until recent works \cite{topping_understanding_2021, velickovic_geometric_2021}, received much less attention. In this work, we investigate graph structure, in particular its granularity, in the context of quality of graph embedding, subsequently reflected in the quality of the downstream task relying on that embedding. The importance of such an investigation follows from the importance of the quality aspect for the downstream tasks.

Typically, an application of machine learning to graphs has two phases: representation learning, which maps the graph into a Euclidean space, and a downstream task, such as classification, regression, or clustering. The first phase has very high computational demands, which can be substantially decreased with graph coarsening. However, it is known that there is an interplay between coarsening and the quality of embedding \cite{akyildiz_understanding_2020, makarov_survey_2021}, which in turn entails an interplay between coarsening and the quality of the downstream task.

Our work builds on the HARP \cite{chen_harp_2018} method for pretraining on coarsened graphs. In HARP, a graph is repeatedly coarsened and the coarser graphs are then used in reverse order (from coarsest to finest) to pre-train a graph representation learning algorithm. While HARP itself works with and modifies the graph structure, this is not the main interest of its authors and it does not include any adaptation to the quality of the obtained result as the authors focus more on the representation and classification accuracy for the original graph. In our work, we modify and generalize the HARP framework to closely study the relationship between graph coarsenings and graph quality in terms of the performance of a downstream task. In our particular case, we chose transductive node classification as the final task, however, the presented algorithms are general graph representation learners that can be utilized for a wide variety of tasks.

The main contributions of this paper are the general framework for graph coarsening and extensions of the HARP algorithm. We extend both main parts of the algorithm in order to observe the effect of graph coarsening to model quality on a fine level. We present two alternative graph coarsening schemes based on graph diffusion convolution and evolutionary algorithms. Additionally, we present a novel way for un-coarsening the reduced graph in a targeted way, based on the confidence of downstream classification for particular nodes. This method maximizes performance under limited graph size.

In the next section, related work is discussed, with the exception of HARP, which is the basis of the reported research, therefore is addressed in detail in Section \ref{sec:harp-framework}, together with our proposal how to extend it into a general framework for graph coarsening. Section \ref{sec:our-method} is the core of this work, presenting our extension of the prolongation step, as well as several proposed alternative graph coarsening schemes. Finally, all proposals are experimentally verified and compared in Section \ref{sec:experimental-evaluation}.
