\section{Introduction}
\todo[inline]{What problem we are solving. Why we are doing this, that the existing methods are too crude, how we are doing much finer work and getting better results}
Across a wide variety of applications and domains, graphs emerge as a domain-independent and ubiquitous way of organizing data. Consequently, machine learning on graphs has, in recent years, seen an explosion in popularity, breadth and depth of both research and applications. While there have been significant advances in algorithms for learning from graph data \cite{kipf_semi-supervised_2016, defferrard_convolutional_2016, chamberlain_grand_2021, li_deepergcn_2021}, the structure of the underlying data has, until recent works such as \cite{topping_understanding_2021} and \cite{velickovic_geometric_2021}, been mostly taken as a given. In this work, we aim to challenge this assumption and take a closer look at the importance of the individual nodes and neighbourhoods that form a graph from the point of view of downstream applications.

Our work builds on the HARP \cite{chen_harp_2018} method for pretraining on coarsened graphs. In HARP, a graph is repeatedly coarsened and the coarser graphs are then used in reverse order (from coarsest to finest) to pre-train a graph representation learning algorithm. While HARP itself works with and modifies the graph structure, this isn't the main interest of the authors of \cite{chen_harp_2018}, who focus more on the obtained representation of the original graph. In our work, we modify and generalize the HARP framework to closely study the relationship between graph coarsenings and graph quality in terms of the performance of a downstream task. In our case, we chose node classification as the ultimate task, however, the presented algorithms are general graph representation learners that can be utilized for a wide variety of tasks.

\subsection{Main contributions}

\subsection{Paper structure}
