\section{Introduction}
Across a wide variety of applications and domains, graphs emerge as a domain-independent and ubiquitous way of organizing structured data. Consequently, machine learning on graphs has, in recent years, seen an explosion in popularity, breadth and depth of both research and applications. While there have been significant advances in algorithms for learning from graph data \cite{defferrard_convolutional_2016,kipf_semi-supervised_2017}, the underlying graph topology has, until recent works \cite{topping_understanding_2021,velickovic_geometric_2021}, received much less attention. In the reported research, we investigate graph granularity in the context of quality of graph embeddings, subsequently reflected in the quality of the downstream tasks relying on those embeddings. The importance of such an investigation follows from the importance of the quality aspect for the downstream tasks.

Typically, an application of machine learning to graphs has two phases: representation learning, which maps the graph into a Euclidean space, and a downstream task, such as classification, regression, or clustering. The first phase has very high computational demands, which can be substantially decreased with graph coarsening, i.e., merging vertices as a means of decreasing the graph size. However, it is known that there is an interplay between coarsening and the quality of embedding \cite{akyildiz_understanding_2020,makarov_survey_2021}, which in turn entails an interplay between coarsening and the performance of the downstream task.

Our work builds on the HARP method for pretraining on coarsened graphs \cite{chen_harp_2018}. In HARP, a graph is repeatedly coarsened and the coarser graphs are then used in reverse order (from coarsest to finest) to pre-train a graph representation learning algorithm. While HARP itself works with and modifies the graph structure, this is not the main interest of its authors and it does not include any adaptation to the quality of the obtained result as the authors focus more on the representation and classification accuracy for the original graph. In our work, we modify and generalize the HARP framework to closely study the relationship between graph coarsenings and graph quality in terms of the performance of a downstream task. In the reported research, we chose transductive node classification as that task, however, the presented algorithms can be utilized for a wide variety of tasks.

The main contributions of this paper are the general framework for graph coarsening and several extensions of the HARP algorithm. We extend both main parts of the algorithm in order to observe the effect of graph coarsening to model quality on a fine level. We present two alternative graph coarsening schemes based on graph diffusion convolution. Additionally, we present a novel approach to refining the reduced graph in a targeted way, based on the confidence of downstream classification. This method maximizes performance under limited graph size.

In the next section, related work is discussed, with the exception of HARP, which is the basis of the reported research, therefore it is addressed in detail in Section~\ref{sec:harp-framework}, together with our proposal how to extend it into a general framework for graph coarsening. Section~\ref{sec:performance-complexity} introduces the addressed performance-complexity trade-off problem in more depth, together with its applicability to graph learning. Section~\ref{sec:our-method} is the core of this work, presenting our extension of the prolongation step, as well as several proposed alternative graph coarsening schemes. Finally, all proposals are experimentally verified and compared in Section~\ref{sec:experimental-evaluation}.
