\section{Introduction}

Graph-based models for machine learning are often used for task with millions of nodes and billions of edges. For such tasks, many machine learning algorithms may be computationally intractable. As a way to combat these growing demands, we look at the trade-off between performance, complexity and method generality. Our work in progress builds on HARP \cite{chen_harp_2018}, a method for pretraining graph-based learning algorithms by first learning on scaled down versions of the graph in question. While in HARP, this downscaling of the graphs is done in a fixed way, we propose a formal generalization of such a downscaling using partially injective homomorphisms, a theoretical result with background in data mining. This generalization would in the future enable the downscaling method to be tailored or trained to the problem at hand. The performance characteristics of HARP are also studied in our work, with the focus being on comparing the performance of models with lower training costs, potentially enabling the use of graph-based algorithms on bigger datasets than previously possible.

In the next section, the graph learning algorithm HARP is presented, followed by a section presenting partially injective homomorphisms. Section \ref{sec:harp-as-pihom} connects these ideas and Section \ref{sec:performance-vs-complexity} discusses the performance characteristics of HARP. Finally, Section \ref{sec:experiments} supports our theses with experimental evaluation.
