\section{HARP and the performance-complexity trade-off}\label{sec:performance-vs-complexity}

Graph-based methods such as node2vec typically have a large number of parameters - on the widely used OGBN-ArXiv dataset (see \cite{hu_open_2021}), the SOTA node2vec model has over 21 million parameters. At the same time, recent works have started to focus more heavily on simpler methods as a competitive alternative to heavy-weight ones (see \cite{frasca_sign_2020,huang_combining_2020,salha_keep_2019,zhang_eigen-gnn_2020}). As the authors of \cite{chen_harp_2018} observed, HARP improves the performance of models when fewer labelled data are available. The proposed lower complexity models based on HARP could also improve performance in a setting where only low fidelity data are available for large parts of the graph. Coarser models could be trained on them, with a subsequent training of finer models using only a limited sample of high fidelity data.

As a core principle of HARP, lower level representations of the task are generated and a model is learnt on them. How good these models are remains in question. In order to test this, several models were compared. Each model \( M_i \) was trained on graphs \( G_L, \dots, G_i \) as with HARP, then the embeddings were prolonged on graphs \( G_{i-1}, \dots, G_0 \) without training. On \( G_0 \), the models were then trained as they would be in an ordinary node2vec setup. With this schema, \( L \) models are obtained, each trained on graphs of different granularity. As the coarsers graphs are much smaller in size, the pretraining is much more efficient to compute that training on the original graph. To examine the performance-complexity trade-off of HARP, the trade-off between decreasing predictive accuracy and decreasing amount of training data was evaluated.
