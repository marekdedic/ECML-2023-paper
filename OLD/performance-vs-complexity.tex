\section{HARP and the performance-complexity trade-off}\label{sec:performance-vs-complexity}


As a core principle of HARP, lower level representations of the task are generated and a model is learnt on them. How good these models are remains in question. In order to test this, several models were compared. Each model \( M_i \) was trained on graphs \( G_L, \dots, G_i \) as with HARP, then the embeddings were prolonged on graphs \( G_{i-1}, \dots, G_0 \) without training. On \( G_0 \), the models were then trained as they would be in an ordinary node2vec setup. With this schema, \( L \) models are obtained, each trained on graphs of different granularity. As the coarsers graphs are much smaller in size, the pretraining is much more efficient to compute that training on the original graph. To examine the performance-complexity trade-off of HARP, the trade-off between decreasing predictive accuracy and decreasing amount of training data was evaluated.
