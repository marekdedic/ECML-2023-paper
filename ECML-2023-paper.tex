%!TEX TS-program = xelatex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[sn-mathphys,pdflatex,iicol]{sn-jnl}% Math and Physical Sciences Reference Style

%%%% Standard Packages
\usepackage{polyglossia} % Must come before biblatex

\usepackage{bm}
\usepackage{csquotes}
\usepackage{fontspec}
\usepackage{hyperref}
%\usepackage{lua-visual-debug}
\usepackage{tabularx}
%%%%

\jyear{2022}

%% as per the requirement new theorem styles can be included as shown below
%%\theoremstyle{thmstyleone}%
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
%%\newtheorem{proposition}[theorem]{Proposition}%
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

%%\theoremstyle{thmstyletwo}%
%%\newtheorem{example}{Example}%
%%\newtheorem{remark}{Remark}%

%%\theoremstyle{thmstylethree}%
%%\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\setdefaultlanguage{english}
\setotherlanguage{czech}

\hypersetup{
	pdfencoding=auto,
	unicode=true,
	bookmarksopen=true,
	bookmarksopenlevel=3
}

\newcommand{\name}[1]{\textit{#1}}
\newcommand{\mathfield}{\ensuremath{\mathbb}}
\newcommand{\mathmat}{\ensuremath{\mathbf}}
\newcommand{\mathset}{\ensuremath{\mathbb}}
\newcommand{\mathspace}{\ensuremath{\mathcal}}
\newcommand{\mathvec}{\ensuremath{\bm}}

\newcounter{enumroman}
\newenvironment{romanitems}{\begin{list}{\bfseries(\roman{enumroman})\hfill}{\usecounter{enumroman}\setlength{\labelwidth}{\leftmargin}\addtolength{\labelwidth}{-1\labelsep}\topsep=0mm plus 2pt\itemsep=0mm\parsep=0mm plus 2pt\itemindent=0mm}}{\end{list}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\title[Adaptive graph coarsening]{Balancing performance and complexity with adaptive graph coarsening}

\author*[1,2]{\fnm{Marek} \sur{Dědič}}\email{marek@dedic.eu}

\author[2]{\fnm{Lukáš} \sur{Bajer}}\email{lubajer@cisco.com}

\author[2]{\fnm{Pavel} \sur{Procházka}}\email{paprocha@cisco.com}
\author[3]{\fnm{Martin} \sur{Holeňa}}\email{martin@cs.cas.cz}

\affil[1]{\orgdiv{Faculty of Nuclear Sciences and Physical Engineering}, \orgname{Czech Technical University in Prague}, \orgaddress{\street{Břehová 7}, \city{Prague}, \postcode{110 00}, \country{Czech Republic}}}

\affil[2]{\orgdiv{Cognitive Intelligence}, \orgname{Cisco Systems, Inc.}, \orgaddress{\street{Karlovo náměstí 10}, \city{Prague}, \postcode{120 00}, \country{Czech Republic}}}

\affil[3]{\orgdiv{Institute of Computer Science}, \orgname{Czech Academy of Sciences}, \orgaddress{\street{Pod vodárenskou věží 2}, \city{Prague}, \postcode{182 07}, \country{Czech Republic}}}

\abstract{
Graph based models are used for tasks with increasing size and computational demands. We present a method for studying graph properties from the point of view of a downstream task. This method allows a user to precisely select the resolution at which the graph in question should be coarsened. Our method builds on an existing algorithm for pretraining on coarser graphs, HARP. We extend both main parts of the algorithm in order to observe the effect of graph coarsening to model quality on a fine level. We present a general framework for graph coarsenings, allowing is to cover, apart from HARP, two alternative algorithms based on graph diffusion convolution. Additionally, we present a novel way for refining the reduced graph in a targeted way based on the downstream classification confidence for particular nodes. Together, these enhancements provide sufficient detail where needed, while collapsing structures where per-node information is not necessary for high model performance.
Hence, the method provides a general meta-model for enhancing graph embedding models such as node2vec. We apply it to several datasets, compare the considered coarsenings on them and discuss the differing behaviour on each of them in the context of their properties.
}

%%================================%%
%% Sample for structured abstract %%
%%================================%%

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
%
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
%
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
%
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

\keywords{
  Graph representation learning,
  Graph coarsening,
  Graph diffusion,
  Graph homophily,
  Performance-complexity trade-off,
  HARP
}

\maketitle

\section{Introduction}
Across a wide variety of applications and domains, graphs emerge as a domain-independent and ubiquitous way of organizing structured data. Consequently, machine learning on graphs has, in recent years, seen an explosion in popularity, breadth and depth of both research and applications. While there have been significant advances in algorithms for learning from graph data \cite{defferrard_convolutional_2016, kipf_semi-supervised_2017}, the underlying graph topology has, until recent works \cite{topping_understanding_2021, velickovic_geometric_2021}, received much less attention. In the reported research, we investigate graph granularity in the context of quality of graph embeddings, subsequently reflected in the quality of the downstream tasks relying on those embeddings. The importance of such an investigation follows from the importance of the quality aspect for the downstream tasks.

Typically, an application of machine learning to graphs has two phases: representation learning, which maps the graph into a Euclidean space, and a downstream task, such as classification, regression, or clustering. The first phase has very high computational demands, which can be substantially decreased with graph coarsening, i.e., merging vertices as a means of decreasing the graph size. However, it is known that there is an interplay between coarsening and the quality of embedding \cite{akyildiz_understanding_2020, makarov_survey_2021}, which in turn entails an interplay between coarsening and the performance of the downstream task.

Our work builds on the HARP method for pretraining on coarsened graphs \cite{chen_harp_2018}. In HARP, a graph is repeatedly coarsened and the coarser graphs are then used in reverse order (from coarsest to finest) to pre-train a graph representation learning algorithm. While HARP itself works with and modifies the graph structure, this is not the main interest of its authors and it does not include any adaptation to the quality of the obtained result as the authors focus more on the representation and classification accuracy for the original graph. In our work, we modify and generalize the HARP framework to closely study the relationship between graph coarsenings and graph quality in terms of the performance of a downstream task. In the reported research, we chose transductive node classification as that task, however, the presented algorithms can be utilized for a wide variety of tasks.

The main contributions of this paper are the general framework for graph coarsening and several extensions of the HARP algorithm. We extend both main parts of the algorithm in order to observe the effect of graph coarsening to model quality on a fine level. We present two alternative graph coarsening schemes based on graph diffusion convolution. Additionally, we present a novel approach to refining the reduced graph in a targeted way, based on the confidence of downstream classification. This method maximizes performance under limited graph size.

In the next section, related work is discussed, with the exception of HARP, which is the basis of the reported research, therefore it is addressed in detail in Section~\ref{sec:harp-framework}, together with our proposal how to extend it into a general framework for graph coarsening. Section~\ref{sec:performance-complexity} introduces the addressed performance-complexity trade-off problem in more depth, together with its applicability to graph learning. Section~\ref{sec:our-method} is the core of this work, presenting our extension of the prolongation step, as well as several proposed alternative graph coarsening schemes. Finally, all proposals are experimentally verified and compared in Section~\ref{sec:experimental-evaluation}.

\section{Related work}

The publications most relevant to our research are \cite{chen_harp_2018}, in which the HARP approach is proposed, and \cite{gasteiger_diffusion_2019}, in which graph coarsening is performed by means of graph diffusion. Because we directly extend, modify or combine those methods, they will be recalled in some detail in Sections \ref{sec:harp} and \ref{sec:gdc-coarsening}. Other important works concerning graph coarsening are \cite{akyildiz_understanding_2020, chen_graph_2022, cai_graph_2022}, which survey numerous coarsening methods, \cite{huang_scaling_2021}, which presents results concerning scalability of graph coarsening, \cite{catalyurek_multithreaded_2012, herrmann_multilevel_2019}, which establish coarsening as a basis for partitioning, and \cite{loukas_graph_2019}, which shows relationships of graph coarsening to properties of the Laplacian. In view of the fact that the HARP approach, which we extend and modify, is a multilevel approach, we paid attention also to the multilevel graph coarsening methods proposed in \cite{bethune_hierarchical_2020, xie_graph_2020, zhang_harp_2021, liu_hierarchical_2021}, among them \cite{zhang_harp_2021} also being inspired by HARP.

In a broader context, our research is related to the more general topic of graph reduction, which apart from graph coarsening includes also graph sparsification. A general framework covering both coarsening and sparsification has been proposed in \cite{bravo_hermsdorff_unifying_2019}. Also of note is the recent work \cite{kammer_space-efficient_2022}, presenting an alternative coarsening approach for planar graphs and \cite{liu_comprehensive_2022}, which sparsifies not only the graph topology, but simultaneously also the features of its nodes and weights of graph neural network used for its embedding. Elaboration of graph coarsening methods in machine learning can built on several decades of their succesful application, such as pairwise aggregation, independent sets, or algebraic distance, in numerical linear algebra \cite{chen_graph_2022}, including in particular multilevel graph coarsening \cite{osei-kuffuor_matrix_2015, ubaru_sampling_2019}.

\section{The performance-complexity trade-off problem}\label{sec:performance-complexity}

The main aim of this work is to explore the performance-complexity characteristics in the context of graph learning, as considered, for example, in \cite{prochazka_downstream_2022}. Consider an undirected graph \( G \) with nodes \( V \left( G \right) \) and edges \( E \left( G \right) \). The result of the graph coarsening part of the algorithm is a sequence of graphs \( G_0, G_1, G_2, \dots, G_L \) where \( G_0 = G \) and \( L \in \mathfield{N} \) is a hyper-parameter of the method.
Given a model \( M \) that operates on graphs, a performance metric \( P \left( G, M \right) \) and a complexity metric \( C \left( G, M \right) \), the sequence \( G_0, G_1, \dots, G_L \) can be plotted in the performance-complexity space, where the original graph \( G_0 \) usually provides the best performance and incurs the most complexity and subsequent coarsened graphs improve complexity and hurt performance. -- see Figure~\ref{fig:performance-complexity} for an illustration.

\begin{figure}
  \centering
  \includegraphics[width=0.45\textwidth]{performance-complexity.pdf}
  \caption{An example of a performance-complexity curve for a sequence of graphs.}
  \label{fig:performance-complexity}
\end{figure}

This performance-complexity characteristics allows for a choice of an optimal \textbf{working point} for the model M -- i.e., the choice of the optimal coarsening level \( G_i \), which directly influences both the performance and the complexity of the model. The choice of the working point is subjective and depends on the particular use-case, downstream task and the environment in which the model is to be deployed. In general, however, two distinct ways of using the characteristics emerge:
\begin{itemize}
  \item Finding the most performant model for a given complexity budget. That is, for a given maximum allowable complexity \( C_\mathrm{max} \), a graph \( G_i \) is to be found that maximizes \( P \left( G_i, M \right) \) while maintaining \( C \left( G_i, M \right) \leq C_\mathrm{max} \).
  \item Finding the least complex model satisfying a performance target. That is, for a given minimum acceptable performance \( P_\mathrm{min} \), a graph \( G_i \) is to be found that minimizes \( C \left( G_i, M \right) \) while maintaining \( P \left( G_i, M \right) \geq P_\mathrm{min} \).
\end{itemize}

\subsection{Choice of the performance and complexity metrics}

The choice of a suitable performace metric \( P \left( G, M \right) \) is dependent on the downstream task in question -- usually, the same metric used to evaluate the downstream model is utilized as the performance metric. In this work, the accuracy of the transductive classification of nodes is chosen as the performance metric.

In practice, two of the most important complexity metrics are computational complexity and memory complexity. These may not only affect the monetary cost and time needed to train a model, but may prevent the application of a considered model outright due to the infeasibility of the given configuration. As shown in \cite{chiang_cluster-gcn_2019}, in the context of graph algorithms, a good proxy for real-world complexity metrics may be the number of edges or the number of nodes in the graph, the latter being the metric used in the rest of this work.

\subsection{Computational complexity of finding the optimal working point}

While the methods proposed in the rest of this work may yield models and graphs with lower computational demands than models using the original graph, the algorithm for finding the optimal working point itself entails running the same complex models on multiple graphs, therefore potentially offsetting any gains from the lower complexity of the model itself. To overcome these potential shortcomings, the following options are considered:
\begin{itemize}
  \item The optimal working point may generalize to datasets other than the one used for the performance-complexity analysis. While this may not in general be said for any two datasets, similar datasets may arise in the practical setting, such as in the domain of computer network security, where a new graph is produced each time the network is scanned and it is reasonable to assume that the working point would be the same for graphs generated in this manner. Generally, this applies especially well in the case of transductive classification where the model needs to be re-trained when the underlying graph changes.
  \item The whole performance-complexity curve is not needed to choose the optimal working point. In the context of this work, the graphs are evaluated in reverse order, i.e. starting with \( G_L \). For the case of a performance target, the evaluation can be stopped after reaching this target. For the other case of a complexity budget, the evaluation is run until the complexity maximum is met, at which point the working point can be selected to maximize performance. In either case, only graphs \( G_b, \dots, G_L \) are evaluated, for some value of \( b \), therefore removing the need to evaluate the model on the graphs with the highest complexity demands.
\end{itemize}

\section{A coarsening framework based on HARP}\label{sec:harp-framework}

In this Section, first, a brief overview of the HARP framework for graph coarsening \cite{chen_harp_2018} is presented, followed by our analysis of the properties of graph coarsenings in general.

\subsection{HARP}\label{sec:harp}
HARP is a method for improving the performance of graph representation learning algorithms such as DeepWalk \cite{perozzi_deepwalk_2014}, node2vec \cite{grover_node2vec_2016}, or, in general, any algorithm that produces embeddings as a distinct output. The method is a combination of dataset augmentation and pre-training based on the general principle that graph-based models train more efficiently on smaller graphs and can thus be pre-trained on a coarsened representation of the graph at hand. Moreover, the coarsened graphs are able to approximate the global properties of the original data, enabling the representations to better encapsulate such a global structure.

The sequence \( G_0, G_1, G_2, \dots, G_L \) introduced in Section~\ref{sec:performance-complexity} is generated one step at a time, that is, graph \( G_i \) is generated from the graph \( G_{i - 1} \) by coarsening it -- lowering the number of nodes and edges while preserving in some sense the general structure of the graph. Following \cite{chen_harp_2018}, let \( \varphi_i \) denote the mapping of \( G_{i - 1} \) such that \( G_i = \varphi_i \left( G_{i - 1} \right) \). In an overview, the method consists of the following steps:

\begin{enumerate}
  \item \textbf{Dataset augmentation}. The graph \( G \) is consecutively reduced in size by the application of several graph coarsening schemas. In each step, the coarsened graph can be viewed as an ever coarser representation of the graph data and its global structure. This step can be run ahead-of-time to produce the resulting coarsened graphs \( G_0, G_1, \dots, G_L \).
\end{enumerate}
After all the coarsened graphs are pre-computed, the method itself can be executed by repeating the following steps on the graphs from the coarsest to the finest (i.e. from \( G_L \) to \( G_0 \)):
\begin{enumerate}\setcounter{enumi}{1}
  \item \textbf{Training on an intermediary graph}. The graph embedding model is trained on \( G_i \), producing \( \Phi_{G_i} \), an embedding of the graph in a Euclidean space.
  \item \textbf{Embedding prolongation}. The embedding \( \Phi_{G_i} \) is prolonged (i.e. modified to accommodate a finer graph) from the current graph to one that is one step closer to \( G_0 \), yielding \( \Phi_{G_{i - 1}} \). This embedding is used as the starting point for training on \( G_{i - 1} \).
\end{enumerate}
These steps are repeated until \( \Phi_{G_0} \) is computed. While the training step of this schema is a straightforward application of one of the aforementioned embedding algorithms to \( G_i \), the particular details of the coarsening and prolongation steps are further explained in Sections \ref{sec:adaptive-prolongation} and \ref{sec:harp-coarsening}.

The first step is independent of the rest of the computation and can be done ahead of time. The last two steps can be seen as a form of pre-training for the model that is to be learned on the original graph. The whole HARP pipeline is demonstrated in Figure~\ref{fig:harp-overview}.

\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{harp-overview.pdf}
  \caption{An overview of the HARP processing pipeline with one level of coarsening}
  \label{fig:harp-overview}
\end{figure*}

\subsection{Properties of graph coarsenings}\label{sec:coarsening-properties}

The HARP framework presents a particular way of producing coarsened graphs. However, we would like to describe any coarsening of a graph in order to be able to explore the set of all coarsened graphs to study the properties of both the applied algorithms, as well as the underlying data.

To properly describe the set of all coarsenings, some assumptions are needed about what constitutes a coarsening. We define a coarsening as a set \( \mathcal{C} \subseteq E \left( G \right) \), i.e. a subset of edges of the original graph such that the graph \( H = \varphi_\mathcal{C} \left( G \right) \) obtained by applying this coarsening to \( G \) is constructed from \( G \) by contracting all edges in \( \mathcal{C} \). Whether to subsequently remove parallel edges arising from this operation is not addressed in the definition and is immaterial to our research. This description of a graph coarsening is inspired by \cite{schulz_mining_2019}, which is why it differs from the weaker definition of a graph reduction as selecting a subset of nodes and edges \cite{huang_scaling_2021, loukas_graph_2019}.

Under such a definition of a graph coarsening, the set of all coarsenings of a graph \( G \) forms a complete bounded lattice with the partial order
\[ \varphi_{\mathcal{C}_1} \preceq \varphi_{\mathcal{C}_2} \iff \mathcal{C}_1 \subseteq \mathcal{C}_2. \]
The least element (bottom) of this lattice is the coarsening \( \bot = \varphi_\emptyset \) where \( \bot \left( G \right) = G \). The greatest element (top) of this lattice is the coarsening \( \top = \varphi_{E \left( G \right)} \), where \( \top \left( G \right) \) is a graph in which there is a single node for each connected component in \( G \). In ideal conditions, to study the effect of graph coarsenings on its quality (viewed through the lens of a downstream task), one would exhaustively study this lattice. In practice, such an exhaustive search is computationally infeasible. With a method such as HARP, the sequence \( \bot, \varphi_1, \dots, \varphi_L, \top \) forms a chain in the lattice, or, more informally, the sequence of coarsenings \( \varphi_1, \dots, \varphi_L \) and the corresponding graph sequence \( G_0, \dots, G_L \) represents a particular way of navigating the lattice of all possible coarsenings between \( \top \) and \( \bot \).

\section{HARP extension for flexible performance-complexity balancing}\label{sec:our-method}

Graph representation learning methods such as node2vec typically have a large number of parameters -- on the widely used OGBN-ArXiv dataset (see \cite{hu_open_2021}), the state-of-the-art node2vec model has over 21 million parameters. At the same time, recent works in the domain of graph learning have started to focus more heavily on simpler methods as a competitive alternative to heavy-weight ones (see \cite{frasca_sign_2020, huang_combining_2020}). As the authors of \cite{chen_harp_2018} observed, HARP improves the performance of models when fewer labelled data are available. The proposed lower complexity models based on HARP could also improve performance in a setting where only low fidelity data are available for large parts of the graph. Coarser models could be trained on them, with a subsequent training of finer models using only a limited sample of high fidelity data.

In this work, we extend the general HARP framework to study the preformance-complexity characteristics of graph data. To this end, we propose alternatives to both the coarsening as well as the prolongation step of HARP. First, in Section~\ref{sec:adaptive-prolongation}, we replace the simple prolongation approach by an adaptive prolongation algorithm. Second, in Section~\ref{sec:coarsening-algorithms}, we study two alternative ways of coarsening the graph.

\subsection{Granularity of the HARP algorithm}

In standard HARP, once the coarsened graphs are obtained, the way to train the graph embedding is fairly straightforward. Starting with the coarsest graph, an embedding model such as node2vec is trained. Following that, a step to a graph that is one level finer is made. The embedding learned on the immediately preceding coarser graph is \name{prolonged} to the embedding of the following finer graph, in which the representations of merged nodes are copied and reused.
 Then, with this prolonged embedding as the starting state, the embedding algorithm continues training and this process is repeated until reaching the original graph.

While this style of prolongation is fine when HARP is used only as a means of pre-training, this approach is far too crude when studying the relationship between graph complexity and the quality of graph embedding and subsequent downstream applications. For example, the widely-used Cora dataset \cite{yang_revisiting_2016} has in its original form 2708 nodes, while the graph resulting from one application of the HARP coarsening schema has only about 1100 nodes (exact numbers may differ run-by-run). Such a relatively high reduction ratio effectively prevents any sufficient understanding of the relationship between graph reduction and changes in the quality of its embedding.

In order to offer a more fine-grained observation of the graph complexity and its effect on the downstream task, we present the adaptive prolongation approach. This algorithm works with the pre-coarsened graphs produced for example by HARP, however, the embedding is learned in a different manner.

\subsection{The adaptive prolongation approach}\label{sec:adaptive-prolongation}

\begin{figure}
  \centering
  \includegraphics[width=0.45\textwidth]{adaptive-prolongation.pdf}
    \caption{A schematic explanation of the adaptive prolongation algorithm for obtaining the embedding \( \Psi_{i} \) from \( \Psi_{i + 1} \).}
  \label{fig:adaptive-prolongation}
\end{figure}

\begin{algorithm*}
  \caption{Adaptive prolongation}
  \label{alg:adaptive-prolongation}
  \begin{algorithmic}
    \Require $ G_0 $ \Comment The original graph
    \Require $ \bm{y}_\mathrm{train} $ \Comment Training labels
    \Require $ n_p $ \Comment The number of nodes to prolong
    \Require $ \Psi_{i + 1} $ \Comment The previous embedding
    \Require $ \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)} $ \Comment A list of all the contractions yet to be reversed
    \Ensure $ \Psi_i $ \Comment The next embedding
    \Ensure $ \mathcal{C}_L^{(i)}, \dots, \mathcal{C}_0^{(i)} $ \Comment Updated contraction list without the prolonged contractions
    \Statex
    \State $ node\_order \gets \Call{get\_node\_order}{G_0, \Psi_{i+1}, \bm{y}_\mathrm{train}, \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)}} $
    \State $ \mathcal{C}_\mathrm{prolong} \gets \Call{select\_contractions}{node\_order, n_p, \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)}} $
    \State $ \Psi_i \gets $ use $ \mathcal{C}_\mathrm{prolong} $ to prolong the embedding $ \Psi_{i + 1} $
    \State $ \mathcal{C}_L^{(i)}, \dots, \mathcal{C}_0^{(i)} \gets $ remove contractions in $ \mathcal{C}_\mathrm{prolong} $ from $ \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)} $
    \Statex
    \Function{get\_node\_order}{$ G_0, \Psi_{i+1}, \bm{y}_\mathrm{train}, \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)} $}
        \State $ \Psi_0^\mathrm{temp} \gets $ use $ \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)} $ to fully prolong the current embedding $ \Psi_{i+1} $ to $ G_0 $
        \State $ model \gets \Call{train\_downstream\_model}{\Psi_0^\mathrm{temp}, \bm{y}_\mathrm{train}} $
        \State $ \mathmat{Y}_\mathrm{pred} \gets \Call{predict}{model, node} $ for each $ node \in V \left( G_0 \right) $
        \State $ entropy\_per\_node \gets H \left( \mathmat{Y}_\mathrm{pred} \right) $
        \State \Return $ V \left( G_0 \right) $, sorted in descending order by $ entropy\_per\_node $
    \EndFunction
    \Statex
    \Function{select\_contractions}{$ ordered\_nodes, n_p, \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)} $}
        \State $ \mathcal{C}_\mathrm{prolong} \gets \left\{ \right\} $
        \For{$ node \in ordered\_nodes $, \textbf{until} $ \left\lvert \mathcal{C}_\mathrm{prolong} \right\rvert = n_p $}
            \State $ contraction \gets \Call{resolve\_contraction}{node, \mathcal{C}_\mathrm{prolong}, \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)}} $
            \State If $ contraction \neq \mathrm{null} $, add $ contraction $ to $ \mathcal{C}_\mathrm{prolong} $
        \EndFor
        \State \Return $ \mathcal{C}_\mathrm{prolong} $
    \EndFunction
    \Statex
    \Function{resolve\_contraction}{$ node, \mathcal{C}_\mathrm{prolong}, \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)} $}
        \State $ contraction \gets \mathrm{null} $
        \For{$ j \in \left\{ 0, \dots, L \right\} $} \Comment I.e. all steps of the original coarsening from finest to coarsest
            \State $ contraction\_candidate \gets $ find in $ \mathcal{C}_j^{(i + 1)} $ a contraction that affects $ node $, if not found, continue with $ j + 1 $
            \If{$ contraction\_candidate \in \mathcal{C}_\mathrm{prolong} $}
                \State \Return $ contraction $
            \EndIf
            \State $ contraction \gets contraction\_candidate $
            \State $ node \gets $ apply $ contraction $ to $ node $, so that in the next loop, a subsequent contraction may be selected
        \EndFor
        \State \Return $ contraction $
    \EndFunction
  \end{algorithmic}
\end{algorithm*}

The adaptive prolongation approach aims to replace the fixed steps defined by the used coarsening algorithm (such as HARP) by a variable number of smaller \enquote{micro-steps}, each of a predefined size that can be chosen independently from the underlying coarsening and its step size. Moreover, the prolongation procedure is driven by the interplay of the downstream task with the local properties of the underlying graph. This enables the method to produce embeddings with different level of granularity in different parts of the graph, e.g. an embedding that is coarse inside clusters of similar nodes and at the same time fine at the border between such clusters.

Let us denote \( \Psi_K, \dots, \Psi_0 \) the embedding sequence produced by the adaptive prolongation approach. To achieve the desired finer control over granularity the sequence \( \Psi_K, \dots, \Psi_0 \) needs to be decoupled from the graph sequence \( G_L, \dots, G_0 \), which in particular implies that the value of \( K \) is independent of \( L \). This is in contrast to the standard HARP prolongation, where the sequence of embeddings \( \Phi_{G_L}, \dots, \Phi_{G_0} \) is directly tied to the previously obtained graphs.

Similarly to standard HARP prolongation, the algorithm starts with the coarsest graph \( G_L \), trains a graph model to compute its embedding \( \Psi_K \) and gradually refines it until reaching the embedding \( \Psi_0 \) of \( G_0 \), or, alternatively, until a stopping criterion is met, as outlined in Section~\ref{sec:performance-complexity}. Instead of directly setting the value of \( K \), a fixed number of nodes \( n_p \) is prolonged in each step. These prolongation steps are interlaid with continued training of the graph model, as in standard HARP.  A description of a single prolongation step from \( \Psi_{i + 1} \) to \( \Psi_i \) follows, is schematically outlined in Figure~\ref{fig:adaptive-prolongation} and described in detail in Algorithm~\ref{alg:adaptive-prolongation}.

The procedure keeps track of all the edge contractions that were made in the dataset augmentation part of the algorithm and gradually reverses them. To this end, apart from the embedding \( \Psi_i \), the set of all contractions yet to be reversed as of step \( i \) is kept as \( \mathcal{C}_L^{(i)}, \dots, \mathcal{C}_0^{(i)} \), with the initial values \( \mathcal{C}_j^{(K)} \) corresponding to the underlying coarsening \( \varphi_j \) as defined in Section~\ref{sec:coarsening-properties}.

In each prolongation step, the embedding \( \Psi_{i + 1} \) is prolonged to \( \Psi_i \) by selecting a set of \( n_p \) contractions \( \mathcal{C}_\mathrm{prolong} \) from the original coarsening procedure and undoing them by copying and reusing the embedding of the node resulting from the contraction to both of the nodes that were contracted. To obtain this set of contractions, nodes of \( G_0 \) are first ordered in such a way that corresponds to the usefulness of prolonging them. Subsequently, the set \( \mathcal{C}_\mathrm{prolong} \) is selected from \( \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)} \) by selecting contractions affecting nodes in the aforementioned order, until \( n_p \) contractions are selected. If multiple contractions affecting the same node are available in the sequence \( \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)} \), one is selected from \( \mathcal{C}_j^{(i + 1)} \) corresponding to the coarsest-level coarsening, that is, from \( \mathcal{C}_j^{(i + 1)} \) with the highest possible \( j \). The sequence \( \mathcal{C}_L^{(i)}, \dots, \mathcal{C}_0^{(i)} \) is produced from \( \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)} \) by removing all of the edges contained in \( \mathcal{C}_\mathrm{prolong} \) (each edge from \( \mathcal{C}_\mathrm{prolong} \) will be contained in exactly one of \( \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)} \)).

To obtain an ordering of nodes of \( G_0 \) based on the usefulness of their prolongation, the embedding \( \Psi_{i + 1} \) is fully prolonged to a temporary embedding of the full graph, \( \Psi_0^\mathrm{temp} \). A downstream model is then trained using this temporary embedding to obtain \( \mathmat{Y}_\mathrm{pred} \), the predicted posterior distribution of classes for each node in \( G_0 \) (e.g. the output of the softmax layer of an MLP). The entropy of this distribution is measured, representing the amount of uncertainty in the classifier for each given node. The nodes are ordered based on the entropy from highest to lowest. This reflects the principle that it is most useful to prolong those nodes where the downstream classifier is the least certain. For downstream tasks other than node classification, the ordering would need to be defined in a different manner, however the approach of prolonging the nodes about which the downstream model is the most uncertain can be extended to other tasks.

\subsection{More general approaches to coarsening}\label{sec:coarsening-algorithms}

While the adaptive prolongation approach substantially generalizes the original method into a powerful tool for studying and leveraging graph structure and its properties under a coarsening, it still relies on the pre-computed coarsenings to guide the prolongation process. Moreover, the general coarsening schema described in Section~\ref{sec:coarsening-properties} and its variants described in this section are useful even beyond the methods proposed in this work. In this section, we first present a brief overview of the coarsening algorithm as proposed by \cite{chen_harp_2018}, followed by two alternative proposals for coarsening construction based on graph diffusion.

\subsubsection{HARP coarsening}\label{sec:harp-coarsening}

The authors of \cite{chen_harp_2018} introduce two particular coarsening methods that together realize the function \( \varphi_i \) from Section~\ref{sec:harp} -- \name{edge collapsing} and \name{star collapsing}. Edge collapsing is a very simple method -- out of all the edges \( E \left( G \right) \), a maximal subset \( E' \) is selected randomly such that no two edges from \( E' \) are incident on the same node. Then, each edge in \( E' \) is contracted.

The edge collapsing algorithm is a good general way of lowering the number of nodes in a graph. However, some structures are not easily collapsed by it. An example of such a structure is a \enquote{star} -- a single node connected to many other nodes. To coarsen graphs with such structures effectively, the star collapsing algorithm is proposed. For each such \textit{hub} node \( u \) in order of decreasing degree, its unconnected neighbouring nodes are taken and merged pairwise. All edges incident on such nodes are replaced with edges incident on the corresponding newly created nodes. As in edge collapsing, nodes to be merged are selected in such a way that no node is merged twice.

These two approaches are combined in the original HARP method, with each coarsening step being a star collapsing step followed by an edge collapsing step. Of a particular note is the fact such a coarsening scheme is not in agreement with the definition presented in Section~\ref{sec:coarsening-properties}. The star collapsing algorithm merges nodes that are adjacent to a common hub node, however, these nodes need not be connected by an edge. In our previous work \cite{dedic_graph_2021}, we experimentally verified that the star collapsing algorithm can be replaced by a similar algorithm that repeatedly merges nodes adjacent on a hub node with the hub node itself. Such a replacement modifies the HARP coarsening scheme to be in line with the definition presented in Section~\ref{sec:coarsening-properties}.

\subsubsection{Graph diffusion coarsening}\label{sec:gdc-coarsening}

Our definition of a graph coarsening requires choosing some edges from the original graph. Intuitively, one way of constructing a graph coarsening would be to merge nodes which are similar and therefore no significant amount of information is lost due to such a coarsening. Following both of these premises, we propose a coarsening based on Graph Diffusion Convolution (GDC) \cite{gasteiger_diffusion_2019} algorithm. GDC is a general graph transformation which, in an overview, constructs for a given graph a new edge set, represented by a so-called sparsified generalized graph diffusion matrix \( \hat{\mathmat{S}} \), which would, in the original setting of the method, be used as a replacement for the adjacency matrix of the graph. To apply this algorithm as a way of coarsening the graph, the edge set obtained by GDC is intersected with the edges of the original graph and the resulting edges contracted in the graph, i.e.,
\[ \mathcal{C} = E \left( G_{\hat{\mathmat{S}}} \right) \cap E \left( G \right), \]
using the notation of a general graph coarsening presented in Section~\ref{sec:coarsening-properties}.

In principle, the authors of GDC define the generalized graph diffusion matrix
\begin{equation}\label{eq:gdc-matrix}
    \mathmat{S} = \sum_{k = 1}^\infty \theta_k \mathmat{T}^k
\end{equation}
such that the power series converges. The parameters \( \theta_k \) together with the generalized transition matrix \( \mathmat{T} \) define the exact way in which the diffusion is achieved. Among the choices for \( \mathmat{T} \) is the random walk transition matrix \( \mathmat{T}_ \mathrm{rw} = \mathmat{A} \mathmat{D}^{-1} \) and the symmetric transition matrix \( \mathmat{T}_\mathrm{sym} = \mathmat{D}^{-\frac{1}{2}} \mathmat{A} \mathmat{D}^{-\frac{1}{2}} \) where \( \mathmat{D} \) is the diagonal matrix of node degrees. Two special cases of this general schema using the random walk transition matrix are the Personalized PageRank algorithm (PPR) \cite{page_pagerank_1999} and the heat kernel \cite{kondor_diffusion_2002}. PPR corresponds to choosing
\[ \theta_k = \alpha \left( 1 - \alpha \right)^k \]
where the parameter \( \alpha \in \left( 0, 1 \right) \) is called the teleport probability. The heat kernel corresponds to choosing
\[ \theta_k = e^{-t} \frac{t^k}{k!} \]
with the parameter \( t \) being called the diffusion time. For both of these special cases, Equation \ref{eq:gdc-matrix} has a closed-form solution.

The result of most diffusion processes (including PPR and the heat kernel) is a dense matrix \( \mathmat{S} \), which then needs to be sparsified. In GDC, two methods of sparsification are considered -- thresholding the matrix values and selecting top-\( k \) entries for each column of the matrix. In any case, the matrix is normalized after sparsification, thus finally producing \( \hat{\mathmat{S}} \).

\section{Experimental evaluation}\label{sec:experimental-evaluation}

\subsection{Experiment setup}

\subsubsection{Datasets}

The proposed methods were experimentally verified on several datasets. The datasets Cora and CiteSeer \cite{yang_revisiting_2016} were used with the \enquote{full} train-test split as in \cite{chen_fastgcn_2018}. Two larger datasets were also used, the PubMed dataset \cite{yang_revisiting_2016} and the DBLP dataset \cite{bojchevski_deep_2018}. In addition, 6 variants of the Twitch dataset \cite{rozemberczki_multi-scale_2021} were used. The basic properties of these datasets are listed in Table~\ref{tab:dataset-sizes}.

\begin{table*}
  \begin{center}
    \begin{minipage}{280pt} % Tune this when the table is changed using the lua-visual-debug package
      \caption{Basic properties of the used datasets}
      \label{tab:dataset-sizes}
      \begin{tabular}{lrrrr}
        \toprule
        \textbf{Dataset} & \textbf{Nodes} & \textbf{Edges} & \textbf{Node features} & \textbf{Classes} \\
        \midrule
        Cora             & 2 708          & 10 556         & 1 433                  & 7                \\
        CiteSeer         & 3 327          & 9 104          & 3 703                  & 6                \\
        PubMed           & 19 717         & 88 648         & 500                    & 3                \\
        DBLP             & 17 716         & 105 734        & 1 639                  & 4                \\
        Twitch-DE        & 9 498          & 315 774        & 128                    & 2                \\
        Twitch-EN        & 7 126          & 77 774         & 128                    & 2                \\
        Twitch-ES        & 4 648          & 123 412        & 128                    & 2                \\
        Twitch-FR        & 6 551          & 231 883        & 128                    & 2                \\
        Twitch-PT        & 1 912          & 64 510         & 128                    & 2                \\
        Twitch-RU        & 4 385          & 78 993         & 128                    & 2                \\
        \bottomrule
      \end{tabular}
    \end{minipage}
  \end{center}
\end{table*}

\subsubsection{Methodology of experiments}

The hyper-parameters for both the node2vec model used for the embedding training and the multi-layer perceptron used for downstream classification were initially set to values used in prior art (see \cite{hu_open_2021, fey_fast_2019}) and then manually fine-tuned for each dataset.

The achitecture of the algorithm was identical accross the dataset, with the only difference being in the values of the hyper-parameters, as listed in Table~\ref{tab:hyperparameter-values}. For the Cora dataset, the node2vec model generated an embedding in \( \mathfield{R}^{128} \) from \( 4 \) random walks of length \( 20 \) for each node with a context window of size \( 5 \). The optimizer ADAM \cite{kingma_adam:_2017} was used with a learning rate of \( 0.01 \) and batches of \( 128 \) samples. The model was trained for \( 5 \) epochs and in each step of the adaptive prolongation, \( 100 \) nodes were prolonged, until reaching the original graph. The MLP classifier using the embeddings featured \( 3 \) linear layers of \( 128 \) neurons with batch normalization after each layer. Each layer was normalized using dropout \cite{srivastava_dropout_2014} with the rate of \( 0.5 \). Finally, a linear layer was used for the class prediction. For the classifier, ADAM with a learning rate of \( 0.01 \) was used for \( 30 \) epochs of training with the cross-entropy loss function. Dataset features weren't used for the classifier training as the aim of this work is to compare the embeddings. The experiment was run \( 10 \) times end-to-end and results averaged. The experiments were implemented using PyTorch \cite{paszke_pytorch_2019} and PyTorch Geometric \cite{fey_fast_2019}.

\begin{table*}
  \begin{center}
    \begin{minipage}{360pt} % Tune this when the table is changed using the lua-visual-debug package
      \caption{Hyper-parameter values used for different datasets}
      \label{tab:hyperparameter-values}
      \begin{tabular}{lrrrrr}
        \toprule
        \textbf{Hyper-parameter} & \textbf{Cora} & \textbf{CiteSeer} & \textbf{PubMed} & \textbf{DBLP} & \textbf{Twitch} \\
        \midrule
        Embedding dimension      & 128           & 32                & 64              & 32            & 128             \\
        \# of random walks       & 4             & 5                 & 3               & 2             & 10              \\
        Random walk length       & 20            & 20                & 40              & 20            & 80              \\
        Context window size      & 5             & 5                 & 20              & 5             & 3               \\
        Node2vec learning rate   & 0.01          & 0.01              & 0.01            & 0.01          & 0.025           \\
        Node2vec batch size      & 128           & 128               & 128             & 128           & 128             \\
        Node2vec epochs          & 5             & 7                 & 1               & 1             & 5               \\
        \# of prolonged nodes    & 100           & 150               & 1000            & 800           & 200             \\
        \# of MLP layers         & 3             & 3                 & 1               & 3             & 2               \\
        MLP hidden layer width   & 128           & 256               & 128             & 256           & 64              \\
        Dropout rate             & 0.5           & 0.5               & 0.5             & 0.5           & 0.5             \\
        MLP learning rate        & 0.01          & 0.01              & 0.01            & 0.01          & 0.01            \\
        MLP epochs               & 30            & 80                & 300             & 300           & 500             \\
        \bottomrule
      \end{tabular}
    \end{minipage}
  \end{center}
\end{table*}

\subsection{Evaluation of the adaptive approach}\label{sec:adaptive-experiments}

In order to study the effect of the adaptive prolongation, the adaptive prolongation method was used to assess the performance of downstream transductive classification at different coarsening levels. A node2vec model as described in the previous section was trained with adaptive prolongation based on coarsenings pre-computed by the HARP coarsening algorithm as described in \ref{sec:harp-coarsening}. For each prolongation step, the intermediary embedding was afterwards fully prolonged to obtain an embedding of the original graph \( G \) (as that is the only graph for which ground-truth labels are available). A classifier was then trained with this embedding as input. This setup allows us to compare classification accuracy at each step of the adaptive prolongation. Figure~\ref{fig:adaptive-coarsening} shows the results of this experiment, compared with a baseline node2vec model (that is, without any coarsening or prolongation) that was trained for the same number of epochs as the total epochs of the adaptive model over all prolongation steps.

\begin{figure*}
  \centering
  \includegraphics[width = \linewidth]{adaptive-coarsening.pdf}
  \caption{Downstream classifier accuracies at different steps of adaptive prolongation using the basic HARP coarsening algorithm. Dashed line shows the baseline node2vec model accuracy. The node count is taken relative to the total node count in each dataset. The results are averaged over multiple runs, with the solid line representing the mean and the shaded area denoting one standard deviation.}
  \label{fig:adaptive-coarsening}
\end{figure*}

The behaviour of the model somewhat differs between the used datasets. For the Cora, CiteSeer, DBLP and PubMed datasets, the model starts from a very low performance, which quickly rises as the model trains for several prolongation steps. The model trained on CiteSeer attains performance comparable to the reference model when approximately half of all nodes are available to it. On the other hand, with Cora, the model slowly approaches the reference model for the whole duration of training, only reaching comparable performance at a point where nearly the whole graph is available to it. Models trained on the two larger datasets, DBLP and PubMed, exhibit a different behaviour in that they briefly reach a global maximum followed by a slight decrease in performance until finally approaching the performance of the reference model in a manner similar to the models trained on Cora and CiteSeer. This behaviour is further discussed in the next section. The Twitch dataset is substantially different, with very high initial performance and only small performance gains with increasing number of nodes available, with different variants of it exhibiting this effect to a different magnitude.

To further study the model properties from a statistical point of view, the results were evaluated at \( k \)-th deciles of the node count of the full graph, for all possible values of \( k \). At each decile, the performance of the model was compared to the baseline node2vec model using the Wilcoxon signed-rank test with the Holm-Bonferroni correction for multiple hypothesis testing with the hypothesis that the models are equivalent. None of the hypotheses were rejected by the test at the 5\% level of significance. We attribute this mainly to the Twitch dataset, for which the performance of the adaptive approach is very good even under heavy coarsening

Following recent best-practice recommendations regarding verifying the statistical validity of results \cite{benavoli_time_2017}, the results were also studied from the point of view of Bayesian estimation. Similarly to the frequentist approach, the performance of the model was compared to that of the baseline model at \( k \)-th deciles of the node count, for all possible values of \( k \). The comparison was done using the Bayesian Wilcoxon signed-rank test \cite{benavoli_bayesian_2014} for 3 different widths of the region of practical equivalence (ROPE), 1\%, 5\% and 10\%. The probabilities that the two models are practically equivalent are listed in Table~\ref{tab:bayesian-adaptive}. Of a particular note is the fact that at 50\% complexity, the models have over a 95\% probability of being within 5 percentage points of performance -- showing that the proposed method may offer a significant complexity reduction in exchange for a relatively minor decrease in performance.

\begin{table}
  \begin{center}
    \begin{minipage}{210pt} % Tune this when the table is changed using the lua-visual-debug package
      \caption{The probabilities that the adaptive approach will be practically equivalent to node2vec when compared on different fractions of the full graph and with different widths of the region of practical equivalence.}
      \label{tab:bayesian-adaptive}
      \begin{tabular}{lrrr}
        \toprule
        \textbf{Nodes} & \textbf{1\% ROPE} & \textbf{5\% ROPE} & \textbf{10\% ROPE} \\
        \midrule
        \textbf{10\%}  & 0.1\%             & 29.1\%            & 57.8\%             \\
        \textbf{20\%}  & 0.2\%             & 39.0\%            & 90.5\%             \\
        \textbf{30\%}  & 0.1\%             & 54.5\%            & 98.6\%             \\
        \textbf{40\%}  & 0.1\%             & 71.0\%            & 100.0\%            \\
        \textbf{50\%}  & 0.9\%             & 95.2\%            & 100.0\%            \\
        \textbf{60\%}  & 8.5\%             & 99.8\%            & 100.0\%            \\
        \textbf{70\%}  & 32.6\%            & 100.0\%           & 100.0\%            \\
        \textbf{80\%}  & 62.1\%            & 100.0\%           & 100.0\%            \\
        \textbf{90\%}  & 67.4\%            & 100.0\%           & 100.0\%            \\
        \textbf{100\%} & 83.9\%            & 100.0\%           & 100.0\%            \\
        \bottomrule
      \end{tabular}
    \end{minipage}
  \end{center}
\end{table}

\subsection{The relationship of the results and the properties of the graph}

When the models for DBLP and PubMed are studied more closely, both reach a local maximum at around 14\% of the graph, followed by a slight decline and gradual approach to the baseline. This suggests a global structure in the data, which the model learns at the point of the local maxima. To investigate this hypothesis and try to explain the behaviour of the model in general, several graph metrics were applied to the graphs generated during the adaptive prolongation algorithm run. All of the metrics were applied in two scenarios -- \enquote{absolute}, where the metric was applied to the graph at a particular step in the prolongation process, and \enquote{incremental}, where the metric was applied to the graph induced by the edge set \( \mathspace{C} \), that is, the set of edges to be contracted at that step.

The metrics used were:
\begin{itemize}
  \item Edge homophily \cite{zhu_beyond_2020} is the fraction of edges connecting nodes of the same class.
  \item Node homophily \cite{pei_geom-gcn_2020} is the fraction of node neighbours having the same class as the node in question, averaged over all nodes.
  \item Class homophily \cite{lim_large_2021} is a variant of node homophily that attempts to modify it in such a way as to make it invariant to the number of classes. The metric measures excess node homophily when compared to a null model where edges are independent of node labels.
  \item Adjusted homophily \cite{platonov_characterizing_2022} is a modification of edge homophily targeted at ensuring it is not not biased towards particular class size distributions and that the metric has a constant maximum attained only for perfectly homophilous graphs.
  \item Balanced accuracy \cite{platonov_characterizing_2022} is a modification of edge homophily that balances each classes' contribution.
  \item Adjusted accuracy \cite{platonov_characterizing_2022} is a modification of balanced accuracy aimed at ensuring a constant baseline.
  \item Label informativeness \cite{platonov_characterizing_2022} is a measure of the influence of a node's neighbours' classes to the label of the node itself.
  \item Global assortativity \cite{newman_mixing_2003} is a measure of the tendency of nodes to connect with other similar nodes, rather than dissimilar nodes.
\end{itemize}

The values of these metrics at different steps of the adaptive prolongation algorithm are shown in Figure~\ref{fig:metrics-absolute} and Figure~\ref{fig:metrics-incremental}. In both the absolute and incremental scenarios, most metrics rise over training until hitting a plateau at the same point as the performance stop increasing. The change is the most pronounced in the global assortativity metric, which actually decreases after this point. This suggests the explanation of the graphs being heterophilic when very coarse (as could be expected), then reaching a point where the global structure of the graph is in place and is then only refined in a local sense. Such a behaviour may introduce nodes which have a different label than their neighbourhoods (a kind of noise in the data), which is a possible explanation for the local minimum in performance.

\begin{figure*}
  \centering
  \includegraphics[width = \linewidth]{metrics-absolute.pdf}
  \caption{Values of the metrics for different datasets at different steps of the adpative prolongation algorithm in the absolute scenario. Overlaid as the dashed line is the accuracy of the downstream classifier.}
  \label{fig:metrics-absolute}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width = \linewidth]{metrics-incremental.pdf}
  \caption{Values of the metrics for different datasets at different steps of the adpative prolongation algorithm in the incremental scenario. Overlaid as the dashed line is the accuracy of the downstream classifier.}
  \label{fig:metrics-incremental}
\end{figure*}

\subsection{Comparison of coarsening approaches}

For GDC coarsening, only the top-\( k \) sparsification method produces reliable results as thresholding leads to instability in the coarsening process, either collapsing the whole graph almost immediately, or not collapsing it at all. As for the other parameters, we followed \cite{gasteiger_diffusion_2019}. Both of the diffusion methods propsed in \cite{gasteiger_diffusion_2019} were implemented with the recommended parameter values, that is the heat kernel was used with the diffusion time \( t = 5 \) and the Personalized PageRank algorithm with the teleport probability \( \alpha = 0.15 \) and the recommended normalization. To be able to compute graph diffusion for larger graphs, approximate diffusion algorithms were used. For PPR, a version of the Andersen algorithm \cite{andersen_local_2006} with \( \epsilon = 10^{-2} \) was used and for heat kernel diffusion, a version of the Kloster-Gleich algorithm \cite{kloster_heat_2014} with \( \epsilon = 10^{-5} \) was used. Both of these algorithms were modified to produce edge weights as part of their output.

\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{coarsening-algorithms.pdf}
  \caption{Downstream classifier accuracies at different steps of adaptive prolongation for different coarsening algorithms. Dashed line shows the baseline node2vec model accuracy. The node count is taken relative to the total node count in each dataset. The results are averaged over multiple runs, with the solid line representing the mean and the shaded area denoting one standard deviation.}
  \label{fig:coarsening-algorithms}
\end{figure*}

The behaviour of the models (Figure~\ref{fig:coarsening-algorithms}) again differs substantially between the datasets. Generally, the methods behave in a similar manner, retaining decent performance even at coarser levels and only improving slightly as more data is available. For Cora, CiteSeer, PubMed and DBLP datasets, the GDC based coarsenings outperform standard HARP at most coarsening levels, with PPR having higher accuracy when there is a difference between the two. This suggests that the GDC algorithm is able to preserve the global structure of the graph better than HARP over successive coarsenings. Twitch-EN and Twitch-PT are the only two datasets where HARP outperforms the diffusion-based methods, with other variants showing either comparable performance of all the approaches, or a preference towards GDC. Of note is the fact that the PPR algorithm only coarsened some variants of the Twitch dataset slightly, a problem not shared by the heat diffusion approach.

Statistical tests similar to the ones described in Section~\ref{sec:adaptive-experiments} were carried out to compare the different models. The Friedmann two-way analysis of ranks with the Holm-Bonferroni correction was used to test the hypothesis that all of the methods are equivalent at \( k \)-th deciles for all possible values of \( k \). This hypothesis was rejected for \( k \in \left\{ 1, 2, 3, 4 \right\} \) with Holm-corrected familywise p-values \( 3.36 \times 10^{-3}, 4.35 \times 10^{-3}, 7.36 \times 10^{-4}, 6.77 \times 10^{-3} \), respectively. For these deciles, a post-hoc pairwise Wilcoxon test was performed between the HARP, PPR and heat kernel methods. At the 5\% significance level, the test did not reject the hypothesis that HARP and the heat kernel are equivalent for any \( k \), rejected the hypothesis that HARP and PPR are equivalent for all values of \( k \in \left\{ 1, 2, 3, 4 \right\} \) with Holm-corrected p-values \( 0.023, 0.023, 0.023, 0.047 \) respectively and rejected the hypothesis that the heat kernel and PPR are equivalent for \( k \in \left\{ 1, 3 \right\} \) with Holm-corrected p-values \( 0.035, 0.046 \) respectively.

The coarsening approaches were also compared using Bayesian estimation, using the Bayesian Wilcoxon signed-rank test in a pairwise fashion for all possible values of \( k \) with a 5\% region of practically equivalence. When comparing HARP to the heat kernel, the two methods had over 98\% probability of being practically equivalent for all values of \( k \). The comparison of HARP and PPR is listed in Table~\ref{tab:bayesian-harp-ppr} and the comparison of the heat kernel and PPR in Table~\ref{tab:bayesian-heat-ppr} The results show that all the methods are practically equivalent when the graph is only slightly coarsened and at higher levels of coarsening, PPR dominates both other methods in the situations where it is available.

\begin{table}
  \begin{center}
    \begin{minipage}{180pt} % Tune this when the table is changed using the lua-visual-debug package
      \caption{Comparison of HARP coarsening and PPR based coarsening based on the Bayesian Wilcoxon signed-rank test. The left an right columns list the probability of each respective method having higher accuracy, while the middle column shows the probability of the 2 methods being practically equivalent with a 5\% ROPE.}
      \label{tab:bayesian-harp-ppr}
      \begin{tabular}{lrrr}
        \toprule
        \textbf{Nodes} & \textbf{HARP} & \textbf{Equivalent} & \textbf{PPR} \\
        \midrule
        \textbf{10\%}  & 0.0\%         & 41.1\%            & 58.9\%         \\
        \textbf{20\%}  & 0.0\%         & 69.1\%            & 30.9\%         \\
        \textbf{30\%}  & 0.0\%         & 84.5\%            & 15.5\%         \\
        \textbf{40\%}  & 0.0\%         & 97.9\%            & 2.2\%          \\
        \textbf{50\%}  & 0.0\%         & 99.8\%            & 0.2\%          \\
        \textbf{60\%}  & 0.0\%         & 100.0\%           & 0.0\%          \\
        \textbf{70\%}  & 0.0\%         & 100.0\%           & 0.0\%          \\
        \textbf{80\%}  & 0.0\%         & 100.0\%           & 0.0\%          \\
        \textbf{90\%}  & 0.0\%         & 100.0\%           & 0.0\%          \\
        \textbf{100\%} & 0.0\%         & 100.0\%           & 0.0\%          \\
        \bottomrule
      \end{tabular}
    \end{minipage}
  \end{center}
\end{table}

\begin{table}
  \begin{center}
    \begin{minipage}{180pt} % Tune this when the table is changed using the lua-visual-debug package
      \caption{Comparison of heat kernel and PPR based coarsenings based on the Bayesian Wilcoxon signed-rank test. The left an right columns list the probability of each respective method having higher accuracy, while the middle column shows the probability of the 2 methods being practically equivalent with a 5\% ROPE.}
      \label{tab:bayesian-heat-ppr}
      \begin{tabular}{lrrr}
        \toprule
        \textbf{Nodes} & \textbf{Heat} & \textbf{Equivalent} & \textbf{PPR} \\
        \midrule
        \textbf{10\%}  & 0.0\%         & 58.1\%            & 41.9\%         \\
        \textbf{20\%}  & 0.0\%         & 89.4\%            & 10.6\%         \\
        \textbf{30\%}  & 0.0\%         & 99.9\%            & 0.1\%         \\
        \textbf{40\%}  & 0.0\%         & 100.0\%           & 0.0\%          \\
        \textbf{50\%}  & 0.0\%         & 100.0\%           & 0.0\%          \\
        \textbf{60\%}  & 0.0\%         & 100.0\%           & 0.0\%          \\
        \textbf{70\%}  & 0.0\%         & 100.0\%           & 0.0\%          \\
        \textbf{80\%}  & 0.0\%         & 100.0\%           & 0.0\%          \\
        \textbf{90\%}  & 0.0\%         & 100.0\%           & 0.0\%          \\
        \textbf{100\%} & 0.0\%         & 100.0\%           & 0.0\%          \\
        \bottomrule
      \end{tabular}
    \end{minipage}
  \end{center}
\end{table}

\section{Conclusion}

In this work, a general graph coarsening schema modelled on the basis of the HARP algorithm was proposed. A novel approach to prolonging graphs in the HARP setting was presented, yielding an adaptive algorithm that selectively prolongs the graph in a way that maximizes performance of the considered downstream task under limited graph size. Additionally, 2 alternatives to HARP coarsening based on graph diffusion were presented. Together, these two improvements substantially increase the versatility of HARP, turning it from a method for pre-training into a framework for graph reduction. Such a framework enables the study of properties of particular graphs, making it possible to reveal global structures. The framework may be used for lowering computational demands while preserving downstream task performance as well.

All of the proposed methods were experimentally verified. While the behaviour differs between the graphs studied, in general, our experiments reveal that at about 50\% reduction in node count, the accuracy was still reasonably close to the accuracy on a full graph for most datasets. Additionally, the coarsenings based on graph diffusion were shown to outperform the original coarsening, again with the exact difference depending on the particular dataset and coarsening level.

In future work, a simpler, direct way of tackling the performance-complexity trade-off problem for graphs may be studied as an alternative to the approach proposed in this work. Our preliminary exploration of this direction \cite{prochazka_downstream_2022} studies the setting of direct adaptive coarsening, instead of a fixed coarsening followed by adaptive prolongation.

\section*{Declarations}

\subsection*{Funding}

The research reported in this paper has been supported by the German Research Foundation (DFG) funded project 467401796.

Marek Dědič, Lukáš Bajer and Pavel Procházka are employed by Cisco Systems, Inc. Martin Holeňa is employed by the Institute of Computer Science of the Czech Academy of Sciences, the Czech Technical University in Prague and the University of Rostock.

\subsection*{Conflicts of interest/Competing interests}

Martin Holeňa is a reviewer for the 2023 ECML journal track.

\subsection*{Ethics approval}

Not applicable.

\subsection*{Consent to participate}

Not applicable.

\subsection*{Consent for publication}

Not applicable.

\subsection*{Availability of data and material}

All used datasets are publicly available.

\subsection*{Code availability}

All used code is publicly available at \href{https://github.com/marekdedic/adaptive-graph-coarsening}{https://github.com/marekdedic/adaptive-graph-coarsening}.

\subsection*{Authors' contributions}

All authors contributed to the research conception and design. Experiment programming and execution was done by Marek Dědič. Statistical interpretation of the results was done by Marek Dědič and Martin Holeňa. The first draft of the manuscript was written by Marek Dědič and all authors commented on previous versions of the manuscript. All authors read and approved the final manuscript.

\bibliography{zotero}

\end{document}
